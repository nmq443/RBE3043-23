{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b518a953",
   "metadata": {
    "_cell_guid": "02c96650-458c-464f-993f-f3ca4a3e16f0",
    "_uuid": "836f75dd-4f74-4644-acab-ba8fafd31980",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-12-12T16:59:36.538082Z",
     "iopub.status.busy": "2024-12-12T16:59:36.537281Z",
     "iopub.status.idle": "2024-12-12T16:59:48.736492Z",
     "shell.execute_reply": "2024-12-12T16:59:48.735243Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 12.204597,
     "end_time": "2024-12-12T16:59:48.738326",
     "exception": false,
     "start_time": "2024-12-12T16:59:36.533729",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyvirtualdisplay\r\n",
      "  Downloading PyVirtualDisplay-3.0-py3-none-any.whl.metadata (943 bytes)\r\n",
      "Downloading PyVirtualDisplay-3.0-py3-none-any.whl (15 kB)\r\n",
      "Installing collected packages: pyvirtualdisplay\r\n",
      "Successfully installed pyvirtualdisplay-3.0\r\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "!apt install -y python-opengl ffmpeg > /dev/null 2>&1\n",
    "\n",
    "%pip install pyvirtualdisplay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ed457815",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T16:59:48.743716Z",
     "iopub.status.busy": "2024-12-12T16:59:48.743388Z",
     "iopub.status.idle": "2024-12-12T17:00:18.380848Z",
     "shell.execute_reply": "2024-12-12T17:00:18.379741Z"
    },
    "papermill": {
     "duration": 29.642925,
     "end_time": "2024-12-12T17:00:18.383457",
     "exception": false,
     "start_time": "2024-12-12T16:59:48.740532",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gymnasium in /opt/conda/lib/python3.10/site-packages (0.29.0)\r\n",
      "Requirement already satisfied: numpy>=1.21.0 in /opt/conda/lib/python3.10/site-packages (from gymnasium) (1.26.4)\r\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /opt/conda/lib/python3.10/site-packages (from gymnasium) (3.1.0)\r\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in /opt/conda/lib/python3.10/site-packages (from gymnasium) (4.12.2)\r\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in /opt/conda/lib/python3.10/site-packages (from gymnasium) (0.0.4)\r\n",
      "Collecting panda-gym==3.0.7\r\n",
      "  Downloading panda_gym-3.0.7-py3-none-any.whl.metadata (4.3 kB)\r\n",
      "Requirement already satisfied: gymnasium>=0.26 in /opt/conda/lib/python3.10/site-packages (from panda-gym==3.0.7) (0.29.0)\r\n",
      "Collecting pybullet (from panda-gym==3.0.7)\r\n",
      "  Downloading pybullet-3.2.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.8 kB)\r\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from panda-gym==3.0.7) (1.26.4)\r\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from panda-gym==3.0.7) (1.14.1)\r\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /opt/conda/lib/python3.10/site-packages (from gymnasium>=0.26->panda-gym==3.0.7) (3.1.0)\r\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in /opt/conda/lib/python3.10/site-packages (from gymnasium>=0.26->panda-gym==3.0.7) (4.12.2)\r\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in /opt/conda/lib/python3.10/site-packages (from gymnasium>=0.26->panda-gym==3.0.7) (0.0.4)\r\n",
      "Downloading panda_gym-3.0.7-py3-none-any.whl (23 kB)\r\n",
      "Downloading pybullet-3.2.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (103.2 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.2/103.2 MB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hInstalling collected packages: pybullet, panda-gym\r\n",
      "Successfully installed panda-gym-3.0.7 pybullet-3.2.6\r\n",
      "Requirement already satisfied: pybullet==3.2.6 in /opt/conda/lib/python3.10/site-packages (3.2.6)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install gymnasium\n",
    "!pip install panda-gym==3.0.7 \n",
    "!pip install pybullet==3.2.6  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "74921a1d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T17:00:18.391770Z",
     "iopub.status.busy": "2024-12-12T17:00:18.391437Z",
     "iopub.status.idle": "2024-12-12T17:00:18.921232Z",
     "shell.execute_reply": "2024-12-12T17:00:18.920530Z"
    },
    "papermill": {
     "duration": 0.536029,
     "end_time": "2024-12-12T17:00:18.923177",
     "exception": false,
     "start_time": "2024-12-12T17:00:18.387148",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyvirtualdisplay import Display\n",
    "\n",
    "display = Display(visible=0, size=(1024, 768))\n",
    "display.start()\n",
    "\n",
    "from matplotlib import pyplot as plt, animation\n",
    "%matplotlib inline\n",
    "from IPython import display\n",
    "\n",
    "def create_anim(frames, dpi, fps):\n",
    "\n",
    "    plt.figure(figsize=(frames[0].shape[1] / dpi, frames[0].shape[0] / dpi), dpi=dpi)\n",
    "\n",
    "    patch = plt.imshow(frames[0])\n",
    "\n",
    "    def setup():\n",
    "\n",
    "        plt.axis('off')\n",
    "\n",
    "    def animate(i):\n",
    "\n",
    "        patch.set_data(frames[i])\n",
    "\n",
    "    anim = animation.FuncAnimation(plt.gcf(), animate, init_func=setup, frames=len(frames), interval=fps)\n",
    "\n",
    "    return anim\n",
    "\n",
    "\n",
    "def display_anim(frames, dpi=72, fps=50):\n",
    "\n",
    "    anim = create_anim(frames, dpi, fps)\n",
    "\n",
    "    return anim.to_jshtml()\n",
    "\n",
    "\n",
    "def save_anim(frames, filename, dpi=72, fps=50):\n",
    "\n",
    "    anim = create_anim(frames, dpi, fps)\n",
    "\n",
    "    anim.save(filename)\n",
    "\n",
    "\n",
    "class trigger:\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        self._trigger = True\n",
    "\n",
    "    def __call__(self, e):\n",
    "\n",
    "        return self._trigger\n",
    "\n",
    "    def set(self, t):\n",
    "\n",
    "        self._trigger = t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1388db9f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T17:00:18.931002Z",
     "iopub.status.busy": "2024-12-12T17:00:18.930735Z",
     "iopub.status.idle": "2024-12-12T17:00:22.284023Z",
     "shell.execute_reply": "2024-12-12T17:00:22.282946Z"
    },
    "papermill": {
     "duration": 3.359578,
     "end_time": "2024-12-12T17:00:22.286208",
     "exception": false,
     "start_time": "2024-12-12T17:00:18.926630",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'RBE3043-23'...\r\n",
      "remote: Enumerating objects: 437, done.\u001b[K\r\n",
      "remote: Counting objects: 100% (229/229), done.\u001b[K\r\n",
      "remote: Compressing objects: 100% (142/142), done.\u001b[K\r\n",
      "remote: Total 437 (delta 147), reused 149 (delta 87), pack-reused 208 (from 1)\u001b[K\r\n",
      "Receiving objects: 100% (437/437), 18.42 MiB | 21.39 MiB/s, done.\r\n",
      "Resolving deltas: 100% (209/209), done.\r\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/nmq443/RBE3043-23.git\n",
    "import os\n",
    "os.chdir('./RBE3043-23/src')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6488c9cf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T17:00:22.297991Z",
     "iopub.status.busy": "2024-12-12T17:00:22.297322Z",
     "iopub.status.idle": "2024-12-12T17:00:57.430487Z",
     "shell.execute_reply": "2024-12-12T17:00:57.429263Z"
    },
    "papermill": {
     "duration": 35.141868,
     "end_time": "2024-12-12T17:00:57.432725",
     "exception": false,
     "start_time": "2024-12-12T17:00:22.290857",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pybullet build time: Nov 28 2023 23:45:17\r\n",
      "argv[0]=--background_color_red=0.7843137383460999\r\n",
      "argv[1]=--background_color_green=0.7843137383460999\r\n",
      "argv[2]=--background_color_blue=0.7843137383460999\r\n",
      "startThreads creating 1 threads.\r\n",
      "starting thread 0\r\n",
      "started thread 0 \r\n",
      "argc=5\r\n",
      "argv[0] = --unused\r\n",
      "argv[1] = --background_color_red=0.7843137383460999\r\n",
      "argv[2] = --background_color_green=0.7843137383460999\r\n",
      "argv[3] = --background_color_blue=0.7843137383460999\r\n",
      "argv[4] = --start_demo_name=Physics Server\r\n",
      "ExampleBrowserThreadFunc started\r\n",
      "X11 functions dynamically loaded using dlopen/dlsym OK!\r\n",
      "X11 functions dynamically loaded using dlopen/dlsym OK!\r\n",
      "Creating context\r\n",
      "Created GL 3.3 context\r\n",
      "Direct GLX rendering context obtained\r\n",
      "Making context current\r\n",
      "GL_VENDOR=Mesa\r\n",
      "GL_RENDERER=llvmpipe (LLVM 15.0.7, 256 bits)\r\n",
      "GL_VERSION=4.5 (Core Profile) Mesa 23.2.1-1ubuntu3.1~22.04.2\r\n",
      "GL_SHADING_LANGUAGE_VERSION=4.50\r\n",
      "pthread_getconcurrency()=0\r\n",
      "Version = 4.5 (Core Profile) Mesa 23.2.1-1ubuntu3.1~22.04.2\r\n",
      "Vendor = Mesa\r\n",
      "Renderer = llvmpipe (LLVM 15.0.7, 256 bits)\r\n",
      "b3Printf: Selected demo: Physics Server\r\n",
      "startThreads creating 1 threads.\r\n",
      "starting thread 0\r\n",
      "started thread 0 \r\n",
      "MotionThreadFunc thread started\r\n",
      "ven = Mesa\r\n",
      "ven = Mesa\r\n",
      "/kaggle/working/RBE3043-23/src/trainer.py:106: RankWarning: Polyfit may be poorly conditioned\r\n",
      "  coefficients = np.polyfit(\r\n",
      "\r\n",
      "            =========================================\r\n",
      "            Timesteps: 75 / 2,500 (3.0%)\r\n",
      "            Episodes: 1\r\n",
      "            Currently: Rollout\r\n",
      "            Latest Reward: -27\r\n",
      "            Latest Avg Rewards: -27\r\n",
      "            Recent Change: -13.28\r\n",
      "            Best Reward: -26.56\r\n",
      "            Latest Discrete Actor Loss: 0.0\r\n",
      "            Latest Continuous Actor Loss: 0.0\r\n",
      "            Avg Discrete Actor Loss: 0.0\r\n",
      "            Avg Continuous Actor Loss: 0.0\r\n",
      "            Latest Critic Loss: 0.0\r\n",
      "            Avg Critic Loss: 0.0\r\n",
      "            =========================================\r\n",
      "        \r\n",
      "/kaggle/working/RBE3043-23/src/trainer.py:106: RankWarning: Polyfit may be poorly conditioned\r\n",
      "  coefficients = np.polyfit(\r\n",
      "\r\n",
      "            =========================================\r\n",
      "            Timesteps: 75 / 2,500 (3.0%)\r\n",
      "            Episodes: 1\r\n",
      "            Currently: Rollout\r\n",
      "            Latest Reward: -27\r\n",
      "            Latest Avg Rewards: -27\r\n",
      "            Recent Change: -13.28\r\n",
      "            Best Reward: -26.56\r\n",
      "            Latest Discrete Actor Loss: 0.0\r\n",
      "            Latest Continuous Actor Loss: 0.0\r\n",
      "            Avg Discrete Actor Loss: 0.0\r\n",
      "            Avg Continuous Actor Loss: 0.0\r\n",
      "            Latest Critic Loss: 0.0\r\n",
      "            Avg Critic Loss: 0.0\r\n",
      "            =========================================\r\n",
      "        \r\n",
      "/kaggle/working/RBE3043-23/src/trainer.py:414: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\r\n",
      "  advantage = (torch.tensor(rewards, dtype=torch.float32,\r\n",
      "/kaggle/working/RBE3043-23/src/trainer.py:480: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\r\n",
      "  reward_tensor = torch.tensor(rewards, dtype=torch.float32,\r\n",
      "\r\n",
      "            =========================================\r\n",
      "            Timesteps: 150 / 2,500 (6.0%)\r\n",
      "            Episodes: 2\r\n",
      "            Currently: Rollout\r\n",
      "            Latest Reward: -23\r\n",
      "            Latest Avg Rewards: -25\r\n",
      "            Recent Change: -3.07\r\n",
      "            Best Reward: -23.5\r\n",
      "            Latest Discrete Actor Loss: -0.0\r\n",
      "            Latest Continuous Actor Loss: 0.0099\r\n",
      "            Avg Discrete Actor Loss: 0.0\r\n",
      "            Avg Continuous Actor Loss: 0.0099\r\n",
      "            Latest Critic Loss: 278.0505\r\n",
      "            Avg Critic Loss: 278.0505\r\n",
      "            =========================================\r\n",
      "        \r\n",
      "\r\n",
      "            =========================================\r\n",
      "            Timesteps: 150 / 2,500 (6.0%)\r\n",
      "            Episodes: 2\r\n",
      "            Currently: Rollout\r\n",
      "            Latest Reward: -23\r\n",
      "            Latest Avg Rewards: -25\r\n",
      "            Recent Change: -3.07\r\n",
      "            Best Reward: -23.5\r\n",
      "            Latest Discrete Actor Loss: -0.0\r\n",
      "            Latest Continuous Actor Loss: 0.0099\r\n",
      "            Avg Discrete Actor Loss: 0.0\r\n",
      "            Avg Continuous Actor Loss: 0.0099\r\n",
      "            Latest Critic Loss: 278.0505\r\n",
      "            Avg Critic Loss: 278.0505\r\n",
      "            =========================================\r\n",
      "        \r\n",
      "\r\n",
      "            =========================================\r\n",
      "            Timesteps: 225 / 2,500 (9.0%)\r\n",
      "            Episodes: 3\r\n",
      "            Currently: Rollout\r\n",
      "            Latest Reward: -38\r\n",
      "            Latest Avg Rewards: -29\r\n",
      "            Recent Change: 5.75\r\n",
      "            Best Reward: -23.5\r\n",
      "            Latest Discrete Actor Loss: 0.0\r\n",
      "            Latest Continuous Actor Loss: 0.0022\r\n",
      "            Avg Discrete Actor Loss: 0.0\r\n",
      "            Avg Continuous Actor Loss: 0.006\r\n",
      "            Latest Critic Loss: 210.6481\r\n",
      "            Avg Critic Loss: 244.3493\r\n",
      "            =========================================\r\n",
      "        \r\n",
      "\r\n",
      "            =========================================\r\n",
      "            Timesteps: 225 / 2,500 (9.0%)\r\n",
      "            Episodes: 3\r\n",
      "            Currently: Rollout\r\n",
      "            Latest Reward: -38\r\n",
      "            Latest Avg Rewards: -29\r\n",
      "            Recent Change: 5.75\r\n",
      "            Best Reward: -23.5\r\n",
      "            Latest Discrete Actor Loss: 0.0\r\n",
      "            Latest Continuous Actor Loss: 0.0022\r\n",
      "            Avg Discrete Actor Loss: 0.0\r\n",
      "            Avg Continuous Actor Loss: 0.006\r\n",
      "            Latest Critic Loss: 210.6481\r\n",
      "            Avg Critic Loss: 244.3493\r\n",
      "            =========================================\r\n",
      "        \r\n",
      "\r\n",
      "            =========================================\r\n",
      "            Timesteps: 300 / 2,500 (12.0%)\r\n",
      "            Episodes: 4\r\n",
      "            Currently: Rollout\r\n",
      "            Latest Reward: -30\r\n",
      "            Latest Avg Rewards: -29\r\n",
      "            Recent Change: 2.44\r\n",
      "            Best Reward: -23.5\r\n",
      "            Latest Discrete Actor Loss: 0.0\r\n",
      "            Latest Continuous Actor Loss: 0.0041\r\n",
      "            Avg Discrete Actor Loss: 0.0\r\n",
      "            Avg Continuous Actor Loss: 0.0054\r\n",
      "            Latest Critic Loss: 602.9059\r\n",
      "            Avg Critic Loss: 363.8682\r\n",
      "            =========================================\r\n",
      "        \r\n",
      "\r\n",
      "            =========================================\r\n",
      "            Timesteps: 300 / 2,500 (12.0%)\r\n",
      "            Episodes: 4\r\n",
      "            Currently: Rollout\r\n",
      "            Latest Reward: -30\r\n",
      "            Latest Avg Rewards: -29\r\n",
      "            Recent Change: 2.44\r\n",
      "            Best Reward: -23.5\r\n",
      "            Latest Discrete Actor Loss: 0.0\r\n",
      "            Latest Continuous Actor Loss: 0.0041\r\n",
      "            Avg Discrete Actor Loss: 0.0\r\n",
      "            Avg Continuous Actor Loss: 0.0054\r\n",
      "            Latest Critic Loss: 602.9059\r\n",
      "            Avg Critic Loss: 363.8682\r\n",
      "            =========================================\r\n",
      "        \r\n",
      "\r\n",
      "            =========================================\r\n",
      "            Timesteps: 375 / 2,500 (15.0%)\r\n",
      "            Episodes: 5\r\n",
      "            Currently: Rollout\r\n",
      "            Latest Reward: -25\r\n",
      "            Latest Avg Rewards: -29\r\n",
      "            Recent Change: 0.4\r\n",
      "            Best Reward: -23.5\r\n",
      "            Latest Discrete Actor Loss: -0.0\r\n",
      "            Latest Continuous Actor Loss: 0.0012\r\n",
      "            Avg Discrete Actor Loss: -0.0\r\n",
      "            Avg Continuous Actor Loss: 0.0043\r\n",
      "            Latest Critic Loss: 368.4265\r\n",
      "            Avg Critic Loss: 365.0077\r\n",
      "            =========================================\r\n",
      "        \r\n",
      "\r\n",
      "            =========================================\r\n",
      "            Timesteps: 375 / 2,500 (15.0%)\r\n",
      "            Episodes: 5\r\n",
      "            Currently: Rollout\r\n",
      "            Latest Reward: -25\r\n",
      "            Latest Avg Rewards: -29\r\n",
      "            Recent Change: 0.4\r\n",
      "            Best Reward: -23.5\r\n",
      "            Latest Discrete Actor Loss: -0.0\r\n",
      "            Latest Continuous Actor Loss: 0.0012\r\n",
      "            Avg Discrete Actor Loss: -0.0\r\n",
      "            Avg Continuous Actor Loss: 0.0043\r\n",
      "            Latest Critic Loss: 368.4265\r\n",
      "            Avg Critic Loss: 365.0077\r\n",
      "            =========================================\r\n",
      "        \r\n",
      "\r\n",
      "            =========================================\r\n",
      "            Timesteps: 450 / 2,500 (18.0%)\r\n",
      "            Episodes: 6\r\n",
      "            Currently: Rollout\r\n",
      "            Latest Reward: -31\r\n",
      "            Latest Avg Rewards: -29\r\n",
      "            Recent Change: 0.51\r\n",
      "            Best Reward: -23.5\r\n",
      "            Latest Discrete Actor Loss: -0.0\r\n",
      "            Latest Continuous Actor Loss: 0.0016\r\n",
      "            Avg Discrete Actor Loss: -0.0\r\n",
      "            Avg Continuous Actor Loss: 0.0038\r\n",
      "            Latest Critic Loss: 264.6889\r\n",
      "            Avg Critic Loss: 344.944\r\n",
      "            =========================================\r\n",
      "        \r\n",
      "\r\n",
      "            =========================================\r\n",
      "            Timesteps: 450 / 2,500 (18.0%)\r\n",
      "            Episodes: 6\r\n",
      "            Currently: Rollout\r\n",
      "            Latest Reward: -31\r\n",
      "            Latest Avg Rewards: -29\r\n",
      "            Recent Change: 0.51\r\n",
      "            Best Reward: -23.5\r\n",
      "            Latest Discrete Actor Loss: -0.0\r\n",
      "            Latest Continuous Actor Loss: 0.0016\r\n",
      "            Avg Discrete Actor Loss: -0.0\r\n",
      "            Avg Continuous Actor Loss: 0.0038\r\n",
      "            Latest Critic Loss: 264.6889\r\n",
      "            Avg Critic Loss: 344.944\r\n",
      "            =========================================\r\n",
      "        \r\n",
      "\r\n",
      "            =========================================\r\n",
      "            Timesteps: 525 / 2,500 (21.0%)\r\n",
      "            Episodes: 7\r\n",
      "            Currently: Rollout\r\n",
      "            Latest Reward: -19\r\n",
      "            Latest Avg Rewards: -28\r\n",
      "            Recent Change: -0.74\r\n",
      "            Best Reward: -19.12\r\n",
      "            Latest Discrete Actor Loss: -0.0\r\n",
      "            Latest Continuous Actor Loss: 0.0012\r\n",
      "            Avg Discrete Actor Loss: -0.0\r\n",
      "            Avg Continuous Actor Loss: 0.0033\r\n",
      "            Latest Critic Loss: 378.4013\r\n",
      "            Avg Critic Loss: 350.5202\r\n",
      "            =========================================\r\n",
      "        \r\n",
      "\r\n",
      "            =========================================\r\n",
      "            Timesteps: 525 / 2,500 (21.0%)\r\n",
      "            Episodes: 7\r\n",
      "            Currently: Rollout\r\n",
      "            Latest Reward: -19\r\n",
      "            Latest Avg Rewards: -28\r\n",
      "            Recent Change: -0.74\r\n",
      "            Best Reward: -19.12\r\n",
      "            Latest Discrete Actor Loss: -0.0\r\n",
      "            Latest Continuous Actor Loss: 0.0012\r\n",
      "            Avg Discrete Actor Loss: -0.0\r\n",
      "            Avg Continuous Actor Loss: 0.0033\r\n",
      "            Latest Critic Loss: 378.4013\r\n",
      "            Avg Critic Loss: 350.5202\r\n",
      "            =========================================\r\n",
      "        \r\n",
      "Object 0 dropped to the floor\r\n",
      "\r\n",
      "            =========================================\r\n",
      "            Timesteps: 552 / 2,500 (22.08%)\r\n",
      "            Episodes: 8\r\n",
      "            Currently: Rollout\r\n",
      "            Latest Reward: -60\r\n",
      "            Latest Avg Rewards: -32\r\n",
      "            Recent Change: 2.23\r\n",
      "            Best Reward: -19.12\r\n",
      "            Latest Discrete Actor Loss: -0.0\r\n",
      "            Latest Continuous Actor Loss: 0.0041\r\n",
      "            Avg Discrete Actor Loss: -0.0\r\n",
      "            Avg Continuous Actor Loss: 0.0035\r\n",
      "            Latest Critic Loss: 148.8645\r\n",
      "            Avg Critic Loss: 321.7122\r\n",
      "            =========================================\r\n",
      "        \r\n",
      "\r\n",
      "            =========================================\r\n",
      "            Timesteps: 552 / 2,500 (22.08%)\r\n",
      "            Episodes: 8\r\n",
      "            Currently: Rollout\r\n",
      "            Latest Reward: -60\r\n",
      "            Latest Avg Rewards: -32\r\n",
      "            Recent Change: 2.23\r\n",
      "            Best Reward: -19.12\r\n",
      "            Latest Discrete Actor Loss: -0.0\r\n",
      "            Latest Continuous Actor Loss: 0.0041\r\n",
      "            Avg Discrete Actor Loss: -0.0\r\n",
      "            Avg Continuous Actor Loss: 0.0035\r\n",
      "            Latest Critic Loss: 148.8645\r\n",
      "            Avg Critic Loss: 321.7122\r\n",
      "            =========================================\r\n",
      "        \r\n",
      "b3Printf: b3Warning[examples/SharedMemory/PhysicsClientSharedMemory.cpp,1343]:\r\n",
      "\r\n",
      "b3Printf: Removing body failed\r\n",
      "\r\n",
      "            =========================================\r\n",
      "            Timesteps: 627 / 2,500 (25.08%)\r\n",
      "            Episodes: 9\r\n",
      "            Currently: Rollout\r\n",
      "            Latest Reward: -21\r\n",
      "            Latest Avg Rewards: -30\r\n",
      "            Recent Change: 0.82\r\n",
      "            Best Reward: -19.12\r\n",
      "            Latest Discrete Actor Loss: 0.0\r\n",
      "            Latest Continuous Actor Loss: 0.0007\r\n",
      "            Avg Discrete Actor Loss: 0.0\r\n",
      "            Avg Continuous Actor Loss: 0.0031\r\n",
      "            Latest Critic Loss: 2356.2495\r\n",
      "            Avg Critic Loss: 576.0294\r\n",
      "            =========================================\r\n",
      "        \r\n",
      "\r\n",
      "            =========================================\r\n",
      "            Timesteps: 627 / 2,500 (25.08%)\r\n",
      "            Episodes: 9\r\n",
      "            Currently: Rollout\r\n",
      "            Latest Reward: -21\r\n",
      "            Latest Avg Rewards: -30\r\n",
      "            Recent Change: 0.82\r\n",
      "            Best Reward: -19.12\r\n",
      "            Latest Discrete Actor Loss: 0.0\r\n",
      "            Latest Continuous Actor Loss: 0.0007\r\n",
      "            Avg Discrete Actor Loss: 0.0\r\n",
      "            Avg Continuous Actor Loss: 0.0031\r\n",
      "            Latest Critic Loss: 2356.2495\r\n",
      "            Avg Critic Loss: 576.0294\r\n",
      "            =========================================\r\n",
      "        \r\n",
      "\r\n",
      "            =========================================\r\n",
      "            Timesteps: 702 / 2,500 (28.08%)\r\n",
      "            Episodes: 10\r\n",
      "            Currently: Rollout\r\n",
      "            Latest Reward: -23\r\n",
      "            Latest Avg Rewards: -30\r\n",
      "            Recent Change: 0.17\r\n",
      "            Best Reward: -19.12\r\n",
      "            Latest Discrete Actor Loss: 0.0\r\n",
      "            Latest Continuous Actor Loss: -0.0017\r\n",
      "            Avg Discrete Actor Loss: 0.0\r\n",
      "            Avg Continuous Actor Loss: 0.0026\r\n",
      "            Latest Critic Loss: 172.4174\r\n",
      "            Avg Critic Loss: 531.1836\r\n",
      "            =========================================\r\n",
      "        \r\n",
      "\r\n",
      "            =========================================\r\n",
      "            Timesteps: 702 / 2,500 (28.08%)\r\n",
      "            Episodes: 10\r\n",
      "            Currently: Rollout\r\n",
      "            Latest Reward: -23\r\n",
      "            Latest Avg Rewards: -30\r\n",
      "            Recent Change: 0.17\r\n",
      "            Best Reward: -19.12\r\n",
      "            Latest Discrete Actor Loss: 0.0\r\n",
      "            Latest Continuous Actor Loss: -0.0017\r\n",
      "            Avg Discrete Actor Loss: 0.0\r\n",
      "            Avg Continuous Actor Loss: 0.0026\r\n",
      "            Latest Critic Loss: 172.4174\r\n",
      "            Avg Critic Loss: 531.1836\r\n",
      "            =========================================\r\n",
      "        \r\n",
      "\r\n",
      "            =========================================\r\n",
      "            Timesteps: 777 / 2,500 (31.08%)\r\n",
      "            Episodes: 11\r\n",
      "            Currently: Rollout\r\n",
      "            Latest Reward: -23\r\n",
      "            Latest Avg Rewards: -29\r\n",
      "            Recent Change: -0.16\r\n",
      "            Best Reward: -19.12\r\n",
      "            Latest Discrete Actor Loss: -0.0\r\n",
      "            Latest Continuous Actor Loss: 0.0091\r\n",
      "            Avg Discrete Actor Loss: 0.0\r\n",
      "            Avg Continuous Actor Loss: 0.0032\r\n",
      "            Latest Critic Loss: 213.7585\r\n",
      "            Avg Critic Loss: 499.4411\r\n",
      "            =========================================\r\n",
      "        \r\n",
      "\r\n",
      "            =========================================\r\n",
      "            Timesteps: 777 / 2,500 (31.08%)\r\n",
      "            Episodes: 11\r\n",
      "            Currently: Rollout\r\n",
      "            Latest Reward: -23\r\n",
      "            Latest Avg Rewards: -29\r\n",
      "            Recent Change: -0.16\r\n",
      "            Best Reward: -19.12\r\n",
      "            Latest Discrete Actor Loss: -0.0\r\n",
      "            Latest Continuous Actor Loss: 0.0091\r\n",
      "            Avg Discrete Actor Loss: 0.0\r\n",
      "            Avg Continuous Actor Loss: 0.0032\r\n",
      "            Latest Critic Loss: 213.7585\r\n",
      "            Avg Critic Loss: 499.4411\r\n",
      "            =========================================\r\n",
      "        \r\n",
      "\r\n",
      "            =========================================\r\n",
      "            Timesteps: 852 / 2,500 (34.08%)\r\n",
      "            Episodes: 12\r\n",
      "            Currently: Rollout\r\n",
      "            Latest Reward: -25\r\n",
      "            Latest Avg Rewards: -29\r\n",
      "            Recent Change: -0.28\r\n",
      "            Best Reward: -19.12\r\n",
      "            Latest Discrete Actor Loss: -0.0\r\n",
      "            Latest Continuous Actor Loss: 0.0055\r\n",
      "            Avg Discrete Actor Loss: 0.0\r\n",
      "            Avg Continuous Actor Loss: 0.0034\r\n",
      "            Latest Critic Loss: 211.0096\r\n",
      "            Avg Critic Loss: 473.2201\r\n",
      "            =========================================\r\n",
      "        \r\n",
      "\r\n",
      "            =========================================\r\n",
      "            Timesteps: 852 / 2,500 (34.08%)\r\n",
      "            Episodes: 12\r\n",
      "            Currently: Rollout\r\n",
      "            Latest Reward: -25\r\n",
      "            Latest Avg Rewards: -29\r\n",
      "            Recent Change: -0.28\r\n",
      "            Best Reward: -19.12\r\n",
      "            Latest Discrete Actor Loss: -0.0\r\n",
      "            Latest Continuous Actor Loss: 0.0055\r\n",
      "            Avg Discrete Actor Loss: 0.0\r\n",
      "            Avg Continuous Actor Loss: 0.0034\r\n",
      "            Latest Critic Loss: 211.0096\r\n",
      "            Avg Critic Loss: 473.2201\r\n",
      "            =========================================\r\n",
      "        \r\n",
      "\r\n",
      "            =========================================\r\n",
      "            Timesteps: 927 / 2,500 (37.08%)\r\n",
      "            Episodes: 13\r\n",
      "            Currently: Rollout\r\n",
      "            Latest Reward: -24\r\n",
      "            Latest Avg Rewards: -28\r\n",
      "            Recent Change: -0.37\r\n",
      "            Best Reward: -19.12\r\n",
      "            Latest Discrete Actor Loss: -0.0\r\n",
      "            Latest Continuous Actor Loss: 0.0009\r\n",
      "            Avg Discrete Actor Loss: -0.0\r\n",
      "            Avg Continuous Actor Loss: 0.0032\r\n",
      "            Latest Critic Loss: 240.9057\r\n",
      "            Avg Critic Loss: 453.8605\r\n",
      "            =========================================\r\n",
      "        \r\n",
      "\r\n",
      "            =========================================\r\n",
      "            Timesteps: 927 / 2,500 (37.08%)\r\n",
      "            Episodes: 13\r\n",
      "            Currently: Rollout\r\n",
      "            Latest Reward: -24\r\n",
      "            Latest Avg Rewards: -28\r\n",
      "            Recent Change: -0.37\r\n",
      "            Best Reward: -19.12\r\n",
      "            Latest Discrete Actor Loss: -0.0\r\n",
      "            Latest Continuous Actor Loss: 0.0009\r\n",
      "            Avg Discrete Actor Loss: -0.0\r\n",
      "            Avg Continuous Actor Loss: 0.0032\r\n",
      "            Latest Critic Loss: 240.9057\r\n",
      "            Avg Critic Loss: 453.8605\r\n",
      "            =========================================\r\n",
      "        \r\n",
      "\r\n",
      "            =========================================\r\n",
      "            Timesteps: 1,002 / 2,500 (40.08%)\r\n",
      "            Episodes: 14\r\n",
      "            Currently: Rollout\r\n",
      "            Latest Reward: -31\r\n",
      "            Latest Avg Rewards: -29\r\n",
      "            Recent Change: -0.21\r\n",
      "            Best Reward: -19.12\r\n",
      "            Latest Discrete Actor Loss: 0.0\r\n",
      "            Latest Continuous Actor Loss: 0.0005\r\n",
      "            Avg Discrete Actor Loss: 0.0\r\n",
      "            Avg Continuous Actor Loss: 0.003\r\n",
      "            Latest Critic Loss: 237.4894\r\n",
      "            Avg Critic Loss: 437.2166\r\n",
      "            =========================================\r\n",
      "        \r\n",
      "\r\n",
      "            =========================================\r\n",
      "            Timesteps: 1,002 / 2,500 (40.08%)\r\n",
      "            Episodes: 14\r\n",
      "            Currently: Rollout\r\n",
      "            Latest Reward: -31\r\n",
      "            Latest Avg Rewards: -29\r\n",
      "            Recent Change: -0.21\r\n",
      "            Best Reward: -19.12\r\n",
      "            Latest Discrete Actor Loss: 0.0\r\n",
      "            Latest Continuous Actor Loss: 0.0005\r\n",
      "            Avg Discrete Actor Loss: 0.0\r\n",
      "            Avg Continuous Actor Loss: 0.003\r\n",
      "            Latest Critic Loss: 237.4894\r\n",
      "            Avg Critic Loss: 437.2166\r\n",
      "            =========================================\r\n",
      "        \r\n",
      "\r\n",
      "            =========================================\r\n",
      "            Timesteps: 1,077 / 2,500 (43.08%)\r\n",
      "            Episodes: 15\r\n",
      "            Currently: Rollout\r\n",
      "            Latest Reward: -29\r\n",
      "            Latest Avg Rewards: -29\r\n",
      "            Recent Change: -0.15\r\n",
      "            Best Reward: -19.12\r\n",
      "            Latest Discrete Actor Loss: -0.0\r\n",
      "            Latest Continuous Actor Loss: 0.0021\r\n",
      "            Avg Discrete Actor Loss: -0.0\r\n",
      "            Avg Continuous Actor Loss: 0.003\r\n",
      "            Latest Critic Loss: 384.9923\r\n",
      "            Avg Critic Loss: 433.4863\r\n",
      "            =========================================\r\n",
      "        \r\n",
      "\r\n",
      "            =========================================\r\n",
      "            Timesteps: 1,077 / 2,500 (43.08%)\r\n",
      "            Episodes: 15\r\n",
      "            Currently: Rollout\r\n",
      "            Latest Reward: -29\r\n",
      "            Latest Avg Rewards: -29\r\n",
      "            Recent Change: -0.15\r\n",
      "            Best Reward: -19.12\r\n",
      "            Latest Discrete Actor Loss: -0.0\r\n",
      "            Latest Continuous Actor Loss: 0.0021\r\n",
      "            Avg Discrete Actor Loss: -0.0\r\n",
      "            Avg Continuous Actor Loss: 0.003\r\n",
      "            Latest Critic Loss: 384.9923\r\n",
      "            Avg Critic Loss: 433.4863\r\n",
      "            =========================================\r\n",
      "        \r\n",
      "\r\n",
      "            =========================================\r\n",
      "            Timesteps: 1,152 / 2,500 (46.08%)\r\n",
      "            Episodes: 16\r\n",
      "            Currently: Rollout\r\n",
      "            Latest Reward: -24\r\n",
      "            Latest Avg Rewards: -28\r\n",
      "            Recent Change: -0.24\r\n",
      "            Best Reward: -19.12\r\n",
      "            Latest Discrete Actor Loss: -0.0\r\n",
      "            Latest Continuous Actor Loss: -0.0002\r\n",
      "            Avg Discrete Actor Loss: -0.0\r\n",
      "            Avg Continuous Actor Loss: 0.0027\r\n",
      "            Latest Critic Loss: 357.7623\r\n",
      "            Avg Critic Loss: 428.438\r\n",
      "            =========================================\r\n",
      "        \r\n",
      "\r\n",
      "            =========================================\r\n",
      "            Timesteps: 1,152 / 2,500 (46.08%)\r\n",
      "            Episodes: 16\r\n",
      "            Currently: Rollout\r\n",
      "            Latest Reward: -24\r\n",
      "            Latest Avg Rewards: -28\r\n",
      "            Recent Change: -0.24\r\n",
      "            Best Reward: -19.12\r\n",
      "            Latest Discrete Actor Loss: -0.0\r\n",
      "            Latest Continuous Actor Loss: -0.0002\r\n",
      "            Avg Discrete Actor Loss: -0.0\r\n",
      "            Avg Continuous Actor Loss: 0.0027\r\n",
      "            Latest Critic Loss: 357.7623\r\n",
      "            Avg Critic Loss: 428.438\r\n",
      "            =========================================\r\n",
      "        \r\n",
      "\r\n",
      "            =========================================\r\n",
      "            Timesteps: 1,227 / 2,500 (49.08%)\r\n",
      "            Episodes: 17\r\n",
      "            Currently: Rollout\r\n",
      "            Latest Reward: -19\r\n",
      "            Latest Avg Rewards: -28\r\n",
      "            Recent Change: -0.38\r\n",
      "            Best Reward: -18.75\r\n",
      "            Latest Discrete Actor Loss: -0.0\r\n",
      "            Latest Continuous Actor Loss: 0.0037\r\n",
      "            Avg Discrete Actor Loss: -0.0\r\n",
      "            Avg Continuous Actor Loss: 0.0028\r\n",
      "            Latest Critic Loss: 214.7355\r\n",
      "            Avg Critic Loss: 415.0816\r\n",
      "            =========================================\r\n",
      "        \r\n",
      "\r\n",
      "            =========================================\r\n",
      "            Timesteps: 1,227 / 2,500 (49.08%)\r\n",
      "            Episodes: 17\r\n",
      "            Currently: Rollout\r\n",
      "            Latest Reward: -19\r\n",
      "            Latest Avg Rewards: -28\r\n",
      "            Recent Change: -0.38\r\n",
      "            Best Reward: -18.75\r\n",
      "            Latest Discrete Actor Loss: -0.0\r\n",
      "            Latest Continuous Actor Loss: 0.0037\r\n",
      "            Avg Discrete Actor Loss: -0.0\r\n",
      "            Avg Continuous Actor Loss: 0.0028\r\n",
      "            Latest Critic Loss: 214.7355\r\n",
      "            Avg Critic Loss: 415.0816\r\n",
      "            =========================================\r\n",
      "        \r\n",
      "\r\n",
      "            =========================================\r\n",
      "            Timesteps: 1,302 / 2,500 (52.08%)\r\n",
      "            Episodes: 18\r\n",
      "            Currently: Rollout\r\n",
      "            Latest Reward: -23\r\n",
      "            Latest Avg Rewards: -28\r\n",
      "            Recent Change: -0.4\r\n",
      "            Best Reward: -18.75\r\n",
      "            Latest Discrete Actor Loss: 0.0\r\n",
      "            Latest Continuous Actor Loss: 0.0038\r\n",
      "            Avg Discrete Actor Loss: -0.0\r\n",
      "            Avg Continuous Actor Loss: 0.0029\r\n",
      "            Latest Critic Loss: 137.1512\r\n",
      "            Avg Critic Loss: 398.7328\r\n",
      "            =========================================\r\n",
      "        \r\n",
      "\r\n",
      "            =========================================\r\n",
      "            Timesteps: 1,302 / 2,500 (52.08%)\r\n",
      "            Episodes: 18\r\n",
      "            Currently: Rollout\r\n",
      "            Latest Reward: -23\r\n",
      "            Latest Avg Rewards: -28\r\n",
      "            Recent Change: -0.4\r\n",
      "            Best Reward: -18.75\r\n",
      "            Latest Discrete Actor Loss: 0.0\r\n",
      "            Latest Continuous Actor Loss: 0.0038\r\n",
      "            Avg Discrete Actor Loss: -0.0\r\n",
      "            Avg Continuous Actor Loss: 0.0029\r\n",
      "            Latest Critic Loss: 137.1512\r\n",
      "            Avg Critic Loss: 398.7328\r\n",
      "            =========================================\r\n",
      "        \r\n",
      "\r\n",
      "            =========================================\r\n",
      "            Timesteps: 1,377 / 2,500 (55.08%)\r\n",
      "            Episodes: 19\r\n",
      "            Currently: Rollout\r\n",
      "            Latest Reward: -24\r\n",
      "            Latest Avg Rewards: -27\r\n",
      "            Recent Change: -0.4\r\n",
      "            Best Reward: -18.75\r\n",
      "            Latest Discrete Actor Loss: -0.0\r\n",
      "            Latest Continuous Actor Loss: 0.0038\r\n",
      "            Avg Discrete Actor Loss: -0.0\r\n",
      "            Avg Continuous Actor Loss: 0.0029\r\n",
      "            Latest Critic Loss: 211.0832\r\n",
      "            Avg Critic Loss: 388.3078\r\n",
      "            =========================================\r\n",
      "        \r\n",
      "\r\n",
      "            =========================================\r\n",
      "            Timesteps: 1,377 / 2,500 (55.08%)\r\n",
      "            Episodes: 19\r\n",
      "            Currently: Rollout\r\n",
      "            Latest Reward: -24\r\n",
      "            Latest Avg Rewards: -27\r\n",
      "            Recent Change: -0.4\r\n",
      "            Best Reward: -18.75\r\n",
      "            Latest Discrete Actor Loss: -0.0\r\n",
      "            Latest Continuous Actor Loss: 0.0038\r\n",
      "            Avg Discrete Actor Loss: -0.0\r\n",
      "            Avg Continuous Actor Loss: 0.0029\r\n",
      "            Latest Critic Loss: 211.0832\r\n",
      "            Avg Critic Loss: 388.3078\r\n",
      "            =========================================\r\n",
      "        \r\n",
      "\r\n",
      "            =========================================\r\n",
      "            Timesteps: 1,452 / 2,500 (58.08%)\r\n",
      "            Episodes: 20\r\n",
      "            Currently: Rollout\r\n",
      "            Latest Reward: -34\r\n",
      "            Latest Avg Rewards: -28\r\n",
      "            Recent Change: -0.25\r\n",
      "            Best Reward: -18.75\r\n",
      "            Latest Discrete Actor Loss: 0.0\r\n",
      "            Latest Continuous Actor Loss: 0.0065\r\n",
      "            Avg Discrete Actor Loss: -0.0\r\n",
      "            Avg Continuous Actor Loss: 0.0031\r\n",
      "            Latest Critic Loss: 241.4073\r\n",
      "            Avg Critic Loss: 380.5762\r\n",
      "            =========================================\r\n",
      "        \r\n",
      "\r\n",
      "            =========================================\r\n",
      "            Timesteps: 1,452 / 2,500 (58.08%)\r\n",
      "            Episodes: 20\r\n",
      "            Currently: Rollout\r\n",
      "            Latest Reward: -34\r\n",
      "            Latest Avg Rewards: -28\r\n",
      "            Recent Change: -0.25\r\n",
      "            Best Reward: -18.75\r\n",
      "            Latest Discrete Actor Loss: 0.0\r\n",
      "            Latest Continuous Actor Loss: 0.0065\r\n",
      "            Avg Discrete Actor Loss: -0.0\r\n",
      "            Avg Continuous Actor Loss: 0.0031\r\n",
      "            Latest Critic Loss: 241.4073\r\n",
      "            Avg Critic Loss: 380.5762\r\n",
      "            =========================================\r\n",
      "        \r\n",
      "\r\n",
      "            =========================================\r\n",
      "            Timesteps: 1,527 / 2,500 (61.08%)\r\n",
      "            Episodes: 21\r\n",
      "            Currently: Rollout\r\n",
      "            Latest Reward: -34\r\n",
      "            Latest Avg Rewards: -28\r\n",
      "            Recent Change: -0.14\r\n",
      "            Best Reward: -18.75\r\n",
      "            Latest Discrete Actor Loss: -0.0\r\n",
      "            Latest Continuous Actor Loss: 0.0051\r\n",
      "            Avg Discrete Actor Loss: -0.0\r\n",
      "            Avg Continuous Actor Loss: 0.0032\r\n",
      "            Latest Critic Loss: 457.5833\r\n",
      "            Avg Critic Loss: 384.4265\r\n",
      "            =========================================\r\n",
      "        \r\n",
      "\r\n",
      "            =========================================\r\n",
      "            Timesteps: 1,527 / 2,500 (61.08%)\r\n",
      "            Episodes: 21\r\n",
      "            Currently: Rollout\r\n",
      "            Latest Reward: -34\r\n",
      "            Latest Avg Rewards: -28\r\n",
      "            Recent Change: -0.14\r\n",
      "            Best Reward: -18.75\r\n",
      "            Latest Discrete Actor Loss: -0.0\r\n",
      "            Latest Continuous Actor Loss: 0.0051\r\n",
      "            Avg Discrete Actor Loss: -0.0\r\n",
      "            Avg Continuous Actor Loss: 0.0032\r\n",
      "            Latest Critic Loss: 457.5833\r\n",
      "            Avg Critic Loss: 384.4265\r\n",
      "            =========================================\r\n",
      "        \r\n",
      "Object 0 dropped to the floor\r\n",
      "\r\n",
      "            =========================================\r\n",
      "            Timesteps: 1,552 / 2,500 (62.08%)\r\n",
      "            Episodes: 22\r\n",
      "            Currently: Rollout\r\n",
      "            Latest Reward: -64\r\n",
      "            Latest Avg Rewards: -30\r\n",
      "            Recent Change: 0.3\r\n",
      "            Best Reward: -18.75\r\n",
      "            Latest Discrete Actor Loss: 0.0\r\n",
      "            Latest Continuous Actor Loss: 0.0023\r\n",
      "            Avg Discrete Actor Loss: -0.0\r\n",
      "            Avg Continuous Actor Loss: 0.0032\r\n",
      "            Latest Critic Loss: 457.9038\r\n",
      "            Avg Critic Loss: 387.9255\r\n",
      "            =========================================\r\n",
      "        \r\n",
      "\r\n",
      "            =========================================\r\n",
      "            Timesteps: 1,552 / 2,500 (62.08%)\r\n",
      "            Episodes: 22\r\n",
      "            Currently: Rollout\r\n",
      "            Latest Reward: -64\r\n",
      "            Latest Avg Rewards: -30\r\n",
      "            Recent Change: 0.3\r\n",
      "            Best Reward: -18.75\r\n",
      "            Latest Discrete Actor Loss: 0.0\r\n",
      "            Latest Continuous Actor Loss: 0.0023\r\n",
      "            Avg Discrete Actor Loss: -0.0\r\n",
      "            Avg Continuous Actor Loss: 0.0032\r\n",
      "            Latest Critic Loss: 457.9038\r\n",
      "            Avg Critic Loss: 387.9255\r\n",
      "            =========================================\r\n",
      "        \r\n",
      "b3Printf: b3Warning[examples/SharedMemory/PhysicsClientSharedMemory.cpp,1343]:\r\n",
      "\r\n",
      "b3Printf: Removing body failed\r\n",
      "\r\n",
      "            =========================================\r\n",
      "            Timesteps: 1,627 / 2,500 (65.08%)\r\n",
      "            Episodes: 23\r\n",
      "            Currently: Rollout\r\n",
      "            Latest Reward: -34\r\n",
      "            Latest Avg Rewards: -30\r\n",
      "            Recent Change: 0.32\r\n",
      "            Best Reward: -18.75\r\n",
      "            Latest Discrete Actor Loss: 0.0\r\n",
      "            Latest Continuous Actor Loss: 0.0034\r\n",
      "            Avg Discrete Actor Loss: 0.0\r\n",
      "            Avg Continuous Actor Loss: 0.0032\r\n",
      "            Latest Critic Loss: 2608.1951\r\n",
      "            Avg Critic Loss: 488.8468\r\n",
      "            =========================================\r\n",
      "        \r\n",
      "\r\n",
      "            =========================================\r\n",
      "            Timesteps: 1,627 / 2,500 (65.08%)\r\n",
      "            Episodes: 23\r\n",
      "            Currently: Rollout\r\n",
      "            Latest Reward: -34\r\n",
      "            Latest Avg Rewards: -30\r\n",
      "            Recent Change: 0.32\r\n",
      "            Best Reward: -18.75\r\n",
      "            Latest Discrete Actor Loss: 0.0\r\n",
      "            Latest Continuous Actor Loss: 0.0034\r\n",
      "            Avg Discrete Actor Loss: 0.0\r\n",
      "            Avg Continuous Actor Loss: 0.0032\r\n",
      "            Latest Critic Loss: 2608.1951\r\n",
      "            Avg Critic Loss: 488.8468\r\n",
      "            =========================================\r\n",
      "        \r\n",
      "\r\n",
      "            =========================================\r\n",
      "            Timesteps: 1,702 / 2,500 (68.08%)\r\n",
      "            Episodes: 24\r\n",
      "            Currently: Rollout\r\n",
      "            Latest Reward: -42\r\n",
      "            Latest Avg Rewards: -30\r\n",
      "            Recent Change: 0.4\r\n",
      "            Best Reward: -18.75\r\n",
      "            Latest Discrete Actor Loss: 0.0\r\n",
      "            Latest Continuous Actor Loss: 0.0007\r\n",
      "            Avg Discrete Actor Loss: 0.0\r\n",
      "            Avg Continuous Actor Loss: 0.0031\r\n",
      "            Latest Critic Loss: 476.1075\r\n",
      "            Avg Critic Loss: 488.2929\r\n",
      "            =========================================\r\n",
      "        \r\n",
      "\r\n",
      "            =========================================\r\n",
      "            Timesteps: 1,702 / 2,500 (68.08%)\r\n",
      "            Episodes: 24\r\n",
      "            Currently: Rollout\r\n",
      "            Latest Reward: -42\r\n",
      "            Latest Avg Rewards: -30\r\n",
      "            Recent Change: 0.4\r\n",
      "            Best Reward: -18.75\r\n",
      "            Latest Discrete Actor Loss: 0.0\r\n",
      "            Latest Continuous Actor Loss: 0.0007\r\n",
      "            Avg Discrete Actor Loss: 0.0\r\n",
      "            Avg Continuous Actor Loss: 0.0031\r\n",
      "            Latest Critic Loss: 476.1075\r\n",
      "            Avg Critic Loss: 488.2929\r\n",
      "            =========================================\r\n",
      "        \r\n",
      "\r\n",
      "            =========================================\r\n",
      "            Timesteps: 1,777 / 2,500 (71.08%)\r\n",
      "            Episodes: 25\r\n",
      "            Currently: Rollout\r\n",
      "            Latest Reward: -34\r\n",
      "            Latest Avg Rewards: -30\r\n",
      "            Recent Change: 0.38\r\n",
      "            Best Reward: -18.75\r\n",
      "            Latest Discrete Actor Loss: -0.0\r\n",
      "            Latest Continuous Actor Loss: 0.0023\r\n",
      "            Avg Discrete Actor Loss: 0.0\r\n",
      "            Avg Continuous Actor Loss: 0.003\r\n",
      "            Latest Critic Loss: 708.69\r\n",
      "            Avg Critic Loss: 497.4761\r\n",
      "            =========================================\r\n",
      "        \r\n",
      "\r\n",
      "            =========================================\r\n",
      "            Timesteps: 1,777 / 2,500 (71.08%)\r\n",
      "            Episodes: 25\r\n",
      "            Currently: Rollout\r\n",
      "            Latest Reward: -34\r\n",
      "            Latest Avg Rewards: -30\r\n",
      "            Recent Change: 0.38\r\n",
      "            Best Reward: -18.75\r\n",
      "            Latest Discrete Actor Loss: -0.0\r\n",
      "            Latest Continuous Actor Loss: 0.0023\r\n",
      "            Avg Discrete Actor Loss: 0.0\r\n",
      "            Avg Continuous Actor Loss: 0.003\r\n",
      "            Latest Critic Loss: 708.69\r\n",
      "            Avg Critic Loss: 497.4761\r\n",
      "            =========================================\r\n",
      "        \r\n",
      "\r\n",
      "            =========================================\r\n",
      "            Timesteps: 1,852 / 2,500 (74.08%)\r\n",
      "            Episodes: 26\r\n",
      "            Currently: Rollout\r\n",
      "            Latest Reward: -39\r\n",
      "            Latest Avg Rewards: -31\r\n",
      "            Recent Change: 0.42\r\n",
      "            Best Reward: -18.75\r\n",
      "            Latest Discrete Actor Loss: -0.0\r\n",
      "            Latest Continuous Actor Loss: 0.0011\r\n",
      "            Avg Discrete Actor Loss: 0.0\r\n",
      "            Avg Continuous Actor Loss: 0.003\r\n",
      "            Latest Critic Loss: 468.6104\r\n",
      "            Avg Critic Loss: 496.3215\r\n",
      "            =========================================\r\n",
      "        \r\n",
      "\r\n",
      "            =========================================\r\n",
      "            Timesteps: 1,852 / 2,500 (74.08%)\r\n",
      "            Episodes: 26\r\n",
      "            Currently: Rollout\r\n",
      "            Latest Reward: -39\r\n",
      "            Latest Avg Rewards: -31\r\n",
      "            Recent Change: 0.42\r\n",
      "            Best Reward: -18.75\r\n",
      "            Latest Discrete Actor Loss: -0.0\r\n",
      "            Latest Continuous Actor Loss: 0.0011\r\n",
      "            Avg Discrete Actor Loss: 0.0\r\n",
      "            Avg Continuous Actor Loss: 0.003\r\n",
      "            Latest Critic Loss: 468.6104\r\n",
      "            Avg Critic Loss: 496.3215\r\n",
      "            =========================================\r\n",
      "        \r\n",
      "\r\n",
      "            =========================================\r\n",
      "            Timesteps: 1,927 / 2,500 (77.08%)\r\n",
      "            Episodes: 27\r\n",
      "            Currently: Rollout\r\n",
      "            Latest Reward: -33\r\n",
      "            Latest Avg Rewards: -31\r\n",
      "            Recent Change: 0.39\r\n",
      "            Best Reward: -18.75\r\n",
      "            Latest Discrete Actor Loss: 0.0\r\n",
      "            Latest Continuous Actor Loss: 0.0001\r\n",
      "            Avg Discrete Actor Loss: 0.0\r\n",
      "            Avg Continuous Actor Loss: 0.0028\r\n",
      "            Latest Critic Loss: 636.2934\r\n",
      "            Avg Critic Loss: 501.705\r\n",
      "            =========================================\r\n",
      "        \r\n",
      "\r\n",
      "            =========================================\r\n",
      "            Timesteps: 1,927 / 2,500 (77.08%)\r\n",
      "            Episodes: 27\r\n",
      "            Currently: Rollout\r\n",
      "            Latest Reward: -33\r\n",
      "            Latest Avg Rewards: -31\r\n",
      "            Recent Change: 0.39\r\n",
      "            Best Reward: -18.75\r\n",
      "            Latest Discrete Actor Loss: 0.0\r\n",
      "            Latest Continuous Actor Loss: 0.0001\r\n",
      "            Avg Discrete Actor Loss: 0.0\r\n",
      "            Avg Continuous Actor Loss: 0.0028\r\n",
      "            Latest Critic Loss: 636.2934\r\n",
      "            Avg Critic Loss: 501.705\r\n",
      "            =========================================\r\n",
      "        \r\n",
      "\r\n",
      "            =========================================\r\n",
      "            Timesteps: 2,002 / 2,500 (80.08%)\r\n",
      "            Episodes: 28\r\n",
      "            Currently: Rollout\r\n",
      "            Latest Reward: -29\r\n",
      "            Latest Avg Rewards: -31\r\n",
      "            Recent Change: 0.33\r\n",
      "            Best Reward: -18.75\r\n",
      "            Latest Discrete Actor Loss: -0.0\r\n",
      "            Latest Continuous Actor Loss: 0.0023\r\n",
      "            Avg Discrete Actor Loss: -0.0\r\n",
      "            Avg Continuous Actor Loss: 0.0028\r\n",
      "            Latest Critic Loss: 458.5302\r\n",
      "            Avg Critic Loss: 500.106\r\n",
      "            =========================================\r\n",
      "        \r\n",
      "\r\n",
      "            =========================================\r\n",
      "            Timesteps: 2,002 / 2,500 (80.08%)\r\n",
      "            Episodes: 28\r\n",
      "            Currently: Rollout\r\n",
      "            Latest Reward: -29\r\n",
      "            Latest Avg Rewards: -31\r\n",
      "            Recent Change: 0.33\r\n",
      "            Best Reward: -18.75\r\n",
      "            Latest Discrete Actor Loss: -0.0\r\n",
      "            Latest Continuous Actor Loss: 0.0023\r\n",
      "            Avg Discrete Actor Loss: -0.0\r\n",
      "            Avg Continuous Actor Loss: 0.0028\r\n",
      "            Latest Critic Loss: 458.5302\r\n",
      "            Avg Critic Loss: 500.106\r\n",
      "            =========================================\r\n",
      "        \r\n",
      "\r\n",
      "            =========================================\r\n",
      "            Timesteps: 2,077 / 2,500 (83.08%)\r\n",
      "            Episodes: 29\r\n",
      "            Currently: Rollout\r\n",
      "            Latest Reward: -25\r\n",
      "            Latest Avg Rewards: -31\r\n",
      "            Recent Change: 0.26\r\n",
      "            Best Reward: -18.75\r\n",
      "            Latest Discrete Actor Loss: 0.0\r\n",
      "            Latest Continuous Actor Loss: 0.0016\r\n",
      "            Avg Discrete Actor Loss: -0.0\r\n",
      "            Avg Continuous Actor Loss: 0.0028\r\n",
      "            Latest Critic Loss: 324.6734\r\n",
      "            Avg Critic Loss: 493.8405\r\n",
      "            =========================================\r\n",
      "        \r\n",
      "\r\n",
      "            =========================================\r\n",
      "            Timesteps: 2,077 / 2,500 (83.08%)\r\n",
      "            Episodes: 29\r\n",
      "            Currently: Rollout\r\n",
      "            Latest Reward: -25\r\n",
      "            Latest Avg Rewards: -31\r\n",
      "            Recent Change: 0.26\r\n",
      "            Best Reward: -18.75\r\n",
      "            Latest Discrete Actor Loss: 0.0\r\n",
      "            Latest Continuous Actor Loss: 0.0016\r\n",
      "            Avg Discrete Actor Loss: -0.0\r\n",
      "            Avg Continuous Actor Loss: 0.0028\r\n",
      "            Latest Critic Loss: 324.6734\r\n",
      "            Avg Critic Loss: 493.8405\r\n",
      "            =========================================\r\n",
      "        \r\n",
      "\r\n",
      "            =========================================\r\n",
      "            Timesteps: 2,152 / 2,500 (86.08%)\r\n",
      "            Episodes: 30\r\n",
      "            Currently: Rollout\r\n",
      "            Latest Reward: -26\r\n",
      "            Latest Avg Rewards: -30\r\n",
      "            Recent Change: 0.2\r\n",
      "            Best Reward: -18.75\r\n",
      "            Latest Discrete Actor Loss: 0.0\r\n",
      "            Latest Continuous Actor Loss: -0.0016\r\n",
      "            Avg Discrete Actor Loss: -0.0\r\n",
      "            Avg Continuous Actor Loss: 0.0026\r\n",
      "            Latest Critic Loss: 245.1987\r\n",
      "            Avg Critic Loss: 485.2667\r\n",
      "            =========================================\r\n",
      "        \r\n",
      "\r\n",
      "            =========================================\r\n",
      "            Timesteps: 2,152 / 2,500 (86.08%)\r\n",
      "            Episodes: 30\r\n",
      "            Currently: Rollout\r\n",
      "            Latest Reward: -26\r\n",
      "            Latest Avg Rewards: -30\r\n",
      "            Recent Change: 0.2\r\n",
      "            Best Reward: -18.75\r\n",
      "            Latest Discrete Actor Loss: 0.0\r\n",
      "            Latest Continuous Actor Loss: -0.0016\r\n",
      "            Avg Discrete Actor Loss: -0.0\r\n",
      "            Avg Continuous Actor Loss: 0.0026\r\n",
      "            Latest Critic Loss: 245.1987\r\n",
      "            Avg Critic Loss: 485.2667\r\n",
      "            =========================================\r\n",
      "        \r\n",
      "Object 0 dropped to the floor\r\n",
      "\r\n",
      "            =========================================\r\n",
      "            Timesteps: 2,208 / 2,500 (88.32%)\r\n",
      "            Episodes: 31\r\n",
      "            Currently: Rollout\r\n",
      "            Latest Reward: -81\r\n",
      "            Latest Avg Rewards: -32\r\n",
      "            Recent Change: 0.49\r\n",
      "            Best Reward: -18.75\r\n",
      "            Latest Discrete Actor Loss: -0.0\r\n",
      "            Latest Continuous Actor Loss: 0.0024\r\n",
      "            Avg Discrete Actor Loss: -0.0\r\n",
      "            Avg Continuous Actor Loss: 0.0026\r\n",
      "            Latest Critic Loss: 238.6057\r\n",
      "            Avg Critic Loss: 477.0446\r\n",
      "            =========================================\r\n",
      "        \r\n",
      "\r\n",
      "            =========================================\r\n",
      "            Timesteps: 2,208 / 2,500 (88.32%)\r\n",
      "            Episodes: 31\r\n",
      "            Currently: Rollout\r\n",
      "            Latest Reward: -81\r\n",
      "            Latest Avg Rewards: -32\r\n",
      "            Recent Change: 0.49\r\n",
      "            Best Reward: -18.75\r\n",
      "            Latest Discrete Actor Loss: -0.0\r\n",
      "            Latest Continuous Actor Loss: 0.0024\r\n",
      "            Avg Discrete Actor Loss: -0.0\r\n",
      "            Avg Continuous Actor Loss: 0.0026\r\n",
      "            Latest Critic Loss: 238.6057\r\n",
      "            Avg Critic Loss: 477.0446\r\n",
      "            =========================================\r\n",
      "        \r\n",
      "b3Printf: b3Warning[examples/SharedMemory/PhysicsClientSharedMemory.cpp,1343]:\r\n",
      "\r\n",
      "b3Printf: Removing body failed\r\n",
      "\r\n",
      "            =========================================\r\n",
      "            Timesteps: 2,283 / 2,500 (91.32%)\r\n",
      "            Episodes: 32\r\n",
      "            Currently: Rollout\r\n",
      "            Latest Reward: -25\r\n",
      "            Latest Avg Rewards: -32\r\n",
      "            Recent Change: 0.4\r\n",
      "            Best Reward: -18.75\r\n",
      "            Latest Discrete Actor Loss: 0.0\r\n",
      "            Latest Continuous Actor Loss: 0.0032\r\n",
      "            Avg Discrete Actor Loss: 0.0\r\n",
      "            Avg Continuous Actor Loss: 0.0026\r\n",
      "            Latest Critic Loss: 2721.3157\r\n",
      "            Avg Critic Loss: 549.4405\r\n",
      "            =========================================\r\n",
      "        \r\n",
      "\r\n",
      "            =========================================\r\n",
      "            Timesteps: 2,283 / 2,500 (91.32%)\r\n",
      "            Episodes: 32\r\n",
      "            Currently: Rollout\r\n",
      "            Latest Reward: -25\r\n",
      "            Latest Avg Rewards: -32\r\n",
      "            Recent Change: 0.4\r\n",
      "            Best Reward: -18.75\r\n",
      "            Latest Discrete Actor Loss: 0.0\r\n",
      "            Latest Continuous Actor Loss: 0.0032\r\n",
      "            Avg Discrete Actor Loss: 0.0\r\n",
      "            Avg Continuous Actor Loss: 0.0026\r\n",
      "            Latest Critic Loss: 2721.3157\r\n",
      "            Avg Critic Loss: 549.4405\r\n",
      "            =========================================\r\n",
      "        \r\n",
      "\r\n",
      "            =========================================\r\n",
      "            Timesteps: 2,358 / 2,500 (94.32%)\r\n",
      "            Episodes: 33\r\n",
      "            Currently: Rollout\r\n",
      "            Latest Reward: -25\r\n",
      "            Latest Avg Rewards: -32\r\n",
      "            Recent Change: 0.33\r\n",
      "            Best Reward: -18.75\r\n",
      "            Latest Discrete Actor Loss: -0.0\r\n",
      "            Latest Continuous Actor Loss: 0.0011\r\n",
      "            Avg Discrete Actor Loss: 0.0\r\n",
      "            Avg Continuous Actor Loss: 0.0026\r\n",
      "            Latest Critic Loss: 244.2116\r\n",
      "            Avg Critic Loss: 539.9021\r\n",
      "            =========================================\r\n",
      "        \r\n",
      "\r\n",
      "            =========================================\r\n",
      "            Timesteps: 2,358 / 2,500 (94.32%)\r\n",
      "            Episodes: 33\r\n",
      "            Currently: Rollout\r\n",
      "            Latest Reward: -25\r\n",
      "            Latest Avg Rewards: -32\r\n",
      "            Recent Change: 0.33\r\n",
      "            Best Reward: -18.75\r\n",
      "            Latest Discrete Actor Loss: -0.0\r\n",
      "            Latest Continuous Actor Loss: 0.0011\r\n",
      "            Avg Discrete Actor Loss: 0.0\r\n",
      "            Avg Continuous Actor Loss: 0.0026\r\n",
      "            Latest Critic Loss: 244.2116\r\n",
      "            Avg Critic Loss: 539.9021\r\n",
      "            =========================================\r\n",
      "        \r\n",
      "\r\n",
      "            =========================================\r\n",
      "            Timesteps: 2,433 / 2,500 (97.32%)\r\n",
      "            Episodes: 34\r\n",
      "            Currently: Rollout\r\n",
      "            Latest Reward: -20\r\n",
      "            Latest Avg Rewards: -31\r\n",
      "            Recent Change: 0.25\r\n",
      "            Best Reward: -18.75\r\n",
      "            Latest Discrete Actor Loss: -0.0\r\n",
      "            Latest Continuous Actor Loss: 0.0008\r\n",
      "            Avg Discrete Actor Loss: 0.0\r\n",
      "            Avg Continuous Actor Loss: 0.0025\r\n",
      "            Latest Critic Loss: 233.1277\r\n",
      "            Avg Critic Loss: 530.6059\r\n",
      "            =========================================\r\n",
      "        \r\n",
      "\r\n",
      "            =========================================\r\n",
      "            Timesteps: 2,433 / 2,500 (97.32%)\r\n",
      "            Episodes: 34\r\n",
      "            Currently: Rollout\r\n",
      "            Latest Reward: -20\r\n",
      "            Latest Avg Rewards: -31\r\n",
      "            Recent Change: 0.25\r\n",
      "            Best Reward: -18.75\r\n",
      "            Latest Discrete Actor Loss: -0.0\r\n",
      "            Latest Continuous Actor Loss: 0.0008\r\n",
      "            Avg Discrete Actor Loss: 0.0\r\n",
      "            Avg Continuous Actor Loss: 0.0025\r\n",
      "            Latest Critic Loss: 233.1277\r\n",
      "            Avg Critic Loss: 530.6059\r\n",
      "            =========================================\r\n",
      "        \r\n",
      "\r\n",
      "            =========================================\r\n",
      "            Timesteps: 2,508 / 2,500 (100.32%)\r\n",
      "            Episodes: 35\r\n",
      "            Currently: Rollout\r\n",
      "            Latest Reward: -21\r\n",
      "            Latest Avg Rewards: -31\r\n",
      "            Recent Change: 0.18\r\n",
      "            Best Reward: -18.75\r\n",
      "            Latest Discrete Actor Loss: -0.0\r\n",
      "            Latest Continuous Actor Loss: 0.0022\r\n",
      "            Avg Discrete Actor Loss: 0.0\r\n",
      "            Avg Continuous Actor Loss: 0.0025\r\n",
      "            Latest Critic Loss: 154.9749\r\n",
      "            Avg Critic Loss: 519.5579\r\n",
      "            =========================================\r\n",
      "        \r\n",
      "\r\n",
      "            =========================================\r\n",
      "            Timesteps: 2,508 / 2,500 (100.32%)\r\n",
      "            Episodes: 35\r\n",
      "            Currently: Rollout\r\n",
      "            Latest Reward: -21\r\n",
      "            Latest Avg Rewards: -31\r\n",
      "            Recent Change: 0.18\r\n",
      "            Best Reward: -18.75\r\n",
      "            Latest Discrete Actor Loss: -0.0\r\n",
      "            Latest Continuous Actor Loss: 0.0022\r\n",
      "            Avg Discrete Actor Loss: 0.0\r\n",
      "            Avg Continuous Actor Loss: 0.0025\r\n",
      "            Latest Critic Loss: 154.9749\r\n",
      "            Avg Critic Loss: 519.5579\r\n",
      "            =========================================\r\n",
      "        \r\n",
      "\r\n",
      "Training complete!\r\n",
      "/opt/conda/lib/python3.10/site-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\r\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\r\n",
      "/opt/conda/lib/python3.10/site-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\r\n",
      "  ret = ret.dtype.type(ret / rcount)\r\n",
      "Loss's shape: 35\r\n",
      "Success rate's shape: 35\r\n",
      "Episodes's shape: 35\r\n",
      "numActiveThreads = 0\r\n",
      "stopping threads\r\n",
      "Thread with taskId 0 exiting\r\n",
      "Thread TERMINATED\r\n",
      "destroy semaphore\r\n",
      "semaphore destroyed\r\n",
      "destroy main semaphore\r\n",
      "main semaphore destroyed\r\n",
      "finished\r\n",
      "numActiveThreads = 0\r\n",
      "btShutDownExampleBrowser stopping threads\r\n",
      "Thread with taskId 0 exiting\r\n",
      "Thread TERMINATED\r\n",
      "destroy semaphore\r\n",
      "semaphore destroyed\r\n",
      "destroy main semaphore\r\n",
      "main semaphore destroyed\r\n"
     ]
    }
   ],
   "source": [
    "!python3 train_arm.py"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 30804,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 85.910849,
   "end_time": "2024-12-12T17:01:00.060015",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-12-12T16:59:34.149166",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
