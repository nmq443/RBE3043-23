{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30804,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"3fbfef3d-276c-4def-8f36-5c67c796e803","cell_type":"markdown","source":"# Prepare environment","metadata":{}},{"id":"initial_id","cell_type":"code","source":"!apt install -y python-opengl ffmpeg > /dev/null 2>&1\n\n%pip install pyvirtualdisplay","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T03:33:57.643540Z","iopub.execute_input":"2024-12-09T03:33:57.643946Z","iopub.status.idle":"2024-12-09T03:34:11.484614Z","shell.execute_reply.started":"2024-12-09T03:33:57.643912Z","shell.execute_reply":"2024-12-09T03:34:11.482863Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: pyvirtualdisplay in /opt/conda/lib/python3.10/site-packages (3.0)\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":1},{"id":"5ab89db52a0f728b","cell_type":"code","source":"!pip install gym==0.25.2 \n!pip install gymnasium==1.0.0 \n!pip install imutils==0.5.4 \n!pip install Jinja2==3.1.4 \n!pip install joblib \n!pip install libclang==18.1.1 \n!pip install Markdown==3.7 \n!pip install MarkupSafe==3.0.2 \n!pip install matplotlib==3.9.3 \n!pip install panda-gym==3.0.7 \n!pip install pillow==11.0.0 \n!pip install pybullet==3.2.6 \n!pip install six==1.16.0 \n!pip install sympy==1.13.1 ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T03:34:11.487112Z","iopub.execute_input":"2024-12-09T03:34:11.487524Z","iopub.status.idle":"2024-12-09T03:36:37.228067Z","shell.execute_reply.started":"2024-12-09T03:34:11.487484Z","shell.execute_reply":"2024-12-09T03:36:37.226619Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: gym==0.25.2 in /opt/conda/lib/python3.10/site-packages (0.25.2)\nRequirement already satisfied: numpy>=1.18.0 in /opt/conda/lib/python3.10/site-packages (from gym==0.25.2) (1.26.4)\nRequirement already satisfied: cloudpickle>=1.2.0 in /opt/conda/lib/python3.10/site-packages (from gym==0.25.2) (3.1.0)\nRequirement already satisfied: gym_notices>=0.0.4 in /opt/conda/lib/python3.10/site-packages (from gym==0.25.2) (0.0.8)\nRequirement already satisfied: gymnasium==1.0.0 in /opt/conda/lib/python3.10/site-packages (1.0.0)\nRequirement already satisfied: numpy>=1.21.0 in /opt/conda/lib/python3.10/site-packages (from gymnasium==1.0.0) (1.26.4)\nRequirement already satisfied: cloudpickle>=1.2.0 in /opt/conda/lib/python3.10/site-packages (from gymnasium==1.0.0) (3.1.0)\nRequirement already satisfied: typing-extensions>=4.3.0 in /opt/conda/lib/python3.10/site-packages (from gymnasium==1.0.0) (4.12.2)\nRequirement already satisfied: farama-notifications>=0.0.1 in /opt/conda/lib/python3.10/site-packages (from gymnasium==1.0.0) (0.0.4)\nRequirement already satisfied: imutils==0.5.4 in /opt/conda/lib/python3.10/site-packages (0.5.4)\nRequirement already satisfied: Jinja2==3.1.4 in /opt/conda/lib/python3.10/site-packages (3.1.4)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from Jinja2==3.1.4) (3.0.2)\nRequirement already satisfied: joblib in /opt/conda/lib/python3.10/site-packages (1.4.2)\nRequirement already satisfied: libclang==18.1.1 in /opt/conda/lib/python3.10/site-packages (18.1.1)\nRequirement already satisfied: Markdown==3.7 in /opt/conda/lib/python3.10/site-packages (3.7)\nRequirement already satisfied: MarkupSafe==3.0.2 in /opt/conda/lib/python3.10/site-packages (3.0.2)\nRequirement already satisfied: matplotlib==3.9.3 in /opt/conda/lib/python3.10/site-packages (3.9.3)\nRequirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib==3.9.3) (1.2.1)\nRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib==3.9.3) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib==3.9.3) (4.53.0)\nRequirement already satisfied: kiwisolver>=1.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib==3.9.3) (1.4.5)\nRequirement already satisfied: numpy>=1.23 in /opt/conda/lib/python3.10/site-packages (from matplotlib==3.9.3) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib==3.9.3) (21.3)\nRequirement already satisfied: pillow>=8 in /opt/conda/lib/python3.10/site-packages (from matplotlib==3.9.3) (11.0.0)\nRequirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib==3.9.3) (3.1.2)\nRequirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.10/site-packages (from matplotlib==3.9.3) (2.9.0.post0)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib==3.9.3) (1.16.0)\nRequirement already satisfied: panda-gym==3.0.7 in /opt/conda/lib/python3.10/site-packages (3.0.7)\nRequirement already satisfied: gymnasium>=0.26 in /opt/conda/lib/python3.10/site-packages (from panda-gym==3.0.7) (1.0.0)\nRequirement already satisfied: pybullet in /opt/conda/lib/python3.10/site-packages (from panda-gym==3.0.7) (3.2.6)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from panda-gym==3.0.7) (1.26.4)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from panda-gym==3.0.7) (1.14.1)\nRequirement already satisfied: cloudpickle>=1.2.0 in /opt/conda/lib/python3.10/site-packages (from gymnasium>=0.26->panda-gym==3.0.7) (3.1.0)\nRequirement already satisfied: typing-extensions>=4.3.0 in /opt/conda/lib/python3.10/site-packages (from gymnasium>=0.26->panda-gym==3.0.7) (4.12.2)\nRequirement already satisfied: farama-notifications>=0.0.1 in /opt/conda/lib/python3.10/site-packages (from gymnasium>=0.26->panda-gym==3.0.7) (0.0.4)\nRequirement already satisfied: pillow==11.0.0 in /opt/conda/lib/python3.10/site-packages (11.0.0)\nRequirement already satisfied: pybullet==3.2.6 in /opt/conda/lib/python3.10/site-packages (3.2.6)\nRequirement already satisfied: six==1.16.0 in /opt/conda/lib/python3.10/site-packages (1.16.0)\nRequirement already satisfied: sympy==1.13.1 in /opt/conda/lib/python3.10/site-packages (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy==1.13.1) (1.3.0)\n","output_type":"stream"}],"execution_count":2},{"id":"44801e56de2f4256","cell_type":"code","source":"from pyvirtualdisplay import Display\n\ndisplay = Display(visible=0, size=(1024, 768))\ndisplay.start()\n\nfrom matplotlib import pyplot as plt, animation\n%matplotlib inline\nfrom IPython import display\n\ndef create_anim(frames, dpi, fps):\n\n    plt.figure(figsize=(frames[0].shape[1] / dpi, frames[0].shape[0] / dpi), dpi=dpi)\n\n    patch = plt.imshow(frames[0])\n\n    def setup():\n\n        plt.axis('off')\n\n    def animate(i):\n\n        patch.set_data(frames[i])\n\n    anim = animation.FuncAnimation(plt.gcf(), animate, init_func=setup, frames=len(frames), interval=fps)\n\n    return anim\n\n\ndef display_anim(frames, dpi=72, fps=50):\n\n    anim = create_anim(frames, dpi, fps)\n\n    return anim.to_jshtml()\n\n\ndef save_anim(frames, filename, dpi=72, fps=50):\n\n    anim = create_anim(frames, dpi, fps)\n\n    anim.save(filename)\n\n\nclass trigger:\n\n    def __init__(self):\n\n        self._trigger = True\n\n    def __call__(self, e):\n\n        return self._trigger\n\n    def set(self, t):\n\n        self._trigger = t","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T03:36:37.230031Z","iopub.execute_input":"2024-12-09T03:36:37.230425Z","iopub.status.idle":"2024-12-09T03:36:37.366929Z","shell.execute_reply.started":"2024-12-09T03:36:37.230380Z","shell.execute_reply":"2024-12-09T03:36:37.365657Z"}},"outputs":[],"execution_count":3},{"id":"9140211e-1bcb-422e-bd23-4d448e52510a","cell_type":"markdown","source":"# Clone project to working directory","metadata":{}},{"id":"b8c89f67-0359-4ef3-9469-2520a5e304c2","cell_type":"code","source":"!git clone https://github.com/nmq443/RBE3043-23.git","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T03:36:37.368996Z","iopub.execute_input":"2024-12-09T03:36:37.369336Z","iopub.status.idle":"2024-12-09T03:36:38.550217Z","shell.execute_reply.started":"2024-12-09T03:36:37.369306Z","shell.execute_reply":"2024-12-09T03:36:38.548841Z"}},"outputs":[{"name":"stdout","text":"fatal: destination path 'RBE3043-23' already exists and is not an empty directory.\n","output_type":"stream"}],"execution_count":4},{"id":"61d673a6-6d07-4bde-8786-3400f9433e13","cell_type":"code","source":"import os\nos.chdir('./RBE3043-23/src')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T03:36:38.551877Z","iopub.execute_input":"2024-12-09T03:36:38.552231Z","iopub.status.idle":"2024-12-09T03:36:38.557668Z","shell.execute_reply.started":"2024-12-09T03:36:38.552195Z","shell.execute_reply":"2024-12-09T03:36:38.556546Z"}},"outputs":[],"execution_count":5},{"id":"f8f8ee7c-992c-413d-a8b5-e4d450a7c65d","cell_type":"code","source":"from test_env import *\nfrom model import DiscreteActor, ContinuousActor, Critic\nimport torch\nfrom trainer import Trainer\nfrom os import path\nfrom pathlib import Path\n\nMOVE = 0\nPICK = 1\nPLACE = 2\n\naction_space = {\n    'discrete': {'Move': 0, 'Pick': 1, 'Place': 2},\n    'continuous': [4, 4, 4]\n}\n\ndiscrete_dim = len(action_space['discrete'])\ncontinuous_dim = action_space['continuous']\n\nenv = My_Arm_RobotEnv(\n    observation_type=0,\n    render_mode='rgb_array',\n    blocker_bar=False,\n    objects_count=1,\n    sorting_count=1\n)\n\nobs, _ = env.reset()\nobs_dim = len(obs['observation'])\nd_actor = DiscreteActor(obs_dim=obs_dim, output_dim=discrete_dim)\nc_actor = ContinuousActor(obs_dim=obs_dim,\n                          continuous_param_dim=continuous_dim)\ncritic = Critic(obs_dim=obs_dim)\n\ntrainer = Trainer(\n    env=env,\n    discrete_actor=d_actor,\n    continuous_actor=c_actor,\n    critic=critic,\n    timesteps=2_000_000,\n    timesteps_per_batch=5_000,\n    max_timesteps_per_episode=750,\n)\n\nPath(\"./training\").mkdir(parents=True, exist_ok=True)\nif path.isfile(\"./training/state.data\"):\n    trainer.load(\"./training\")\ntrainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T03:36:38.559025Z","iopub.execute_input":"2024-12-09T03:36:38.559325Z"}},"outputs":[{"name":"stderr","text":"pybullet build time: Nov 28 2023 23:45:17\n","output_type":"stream"},{"name":"stdout","text":"argv[0]=--background_color_red=0.7843137383460999\nargv[1]=--background_color_green=0.7843137383460999\nargv[2]=--background_color_blue=0.7843137383460999\nstartThreads creating 1 threads.\nstarting thread 0\nstarted thread 0 \nargc=5\nargv[0] = --unused\nargv[1] = --background_color_red=0.7843137383460999\nargv[2] = --background_color_green=0.7843137383460999\nargv[3] = --background_color_blue=0.7843137383460999\nargv[4] = --start_demo_name=Physics Server\nExampleBrowserThreadFunc started\nX11 functions dynamically loaded using dlopen/dlsym OK!\nX11 functions dynamically loaded using dlopen/dlsym OK!\nCreating context\nCreated GL 3.3 context\nDirect GLX rendering context obtained\nMaking context current\nGL_VENDOR=Mesa/X.org\nGL_RENDERER=llvmpipe (LLVM 12.0.0, 256 bits)\nGL_VERSION=4.5 (Core Profile) Mesa 21.2.6\nGL_SHADING_LANGUAGE_VERSION=4.50\npthread_getconcurrency()=0\nVersion = 4.5 (Core Profile) Mesa 21.2.6\nVendor = Mesa/X.org\nRenderer = llvmpipe (LLVM 12.0.0, 256 bits)\nb3Printf: Selected demo: Physics Server\nstartThreads creating 1 threads.\nstarting thread 0\nstarted thread 0 \nMotionThreadFunc thread started\nven = Mesa/X.org\nven = Mesa/X.org\n","output_type":"stream"},{"name":"stderr","text":"/kaggle/working/RBE3043-23/src/trainer.py:105: RankWarning: Polyfit may be poorly conditioned\n  coefficients = np.polyfit(\n\n            =========================================\n            Timesteps: 750 / 2,000,000 (0.0375%)\n            Episodes: 1\n            Currently: Rollout\n            Latest Reward: -289\n            Latest Avg Rewards: -289\n            Recent Change: -144.54\n            Best Reward: -289.07\n            Latest Discrete Actor Loss: 0.0\n            Latest Continuous Actor Loss: 0.0\n            Avg Discrete Actor Loss: 0.0\n            Avg Continuous Actor Loss: 0.0\n            Latest Critic Loss: 0.0\n            Avg Critic Loss: 0.0\n            =========================================\n        \n\n            =========================================\n            Timesteps: 1,500 / 2,000,000 (0.075%)\n            Episodes: 2\n            Currently: Rollout\n            Latest Reward: -291\n            Latest Avg Rewards: -290\n            Recent Change: 1.92\n            Best Reward: -289.07\n            Latest Discrete Actor Loss: 0.0\n            Latest Continuous Actor Loss: 0.0\n            Avg Discrete Actor Loss: 0.0\n            Avg Continuous Actor Loss: 0.0\n            Latest Critic Loss: 0.0\n            Avg Critic Loss: 0.0\n            =========================================\n        \n","output_type":"stream"},{"name":"stdout","text":"Object 0 dropped to the floor\nb3Printf: b3Warning[examples/SharedMemory/PhysicsClientSharedMemory.cpp,1343]:\n\nb3Printf: Removing body failed\n","output_type":"stream"},{"name":"stderr","text":"\n            =========================================\n            Timesteps: 1,568 / 2,000,000 (0.0784%)\n            Episodes: 3\n            Currently: Rollout\n            Latest Reward: -76\n            Latest Avg Rewards: -219\n            Recent Change: -106.77\n            Best Reward: -75.53\n            Latest Discrete Actor Loss: 0.0\n            Latest Continuous Actor Loss: 0.0\n            Avg Discrete Actor Loss: 0.0\n            Avg Continuous Actor Loss: 0.0\n            Latest Critic Loss: 0.0\n            Avg Critic Loss: 0.0\n            =========================================\n        \n\n            =========================================\n            Timesteps: 2,318 / 2,000,000 (0.1159%)\n            Episodes: 4\n            Currently: Rollout\n            Latest Reward: -333\n            Latest Avg Rewards: -247\n            Recent Change: -8.28\n            Best Reward: -75.53\n            Latest Discrete Actor Loss: 0.0\n            Latest Continuous Actor Loss: 0.0\n            Avg Discrete Actor Loss: 0.0\n            Avg Continuous Actor Loss: 0.0\n            Latest Critic Loss: 0.0\n            Avg Critic Loss: 0.0\n            =========================================\n        \n\n            =========================================\n            Timesteps: 3,068 / 2,000,000 (0.1534%)\n            Episodes: 5\n            Currently: Rollout\n            Latest Reward: -327\n            Latest Avg Rewards: -263\n            Recent Change: 11.78\n            Best Reward: -75.53\n            Latest Discrete Actor Loss: 0.0\n            Latest Continuous Actor Loss: 0.0\n            Avg Discrete Actor Loss: 0.0\n            Avg Continuous Actor Loss: 0.0\n            Latest Critic Loss: 0.0\n            Avg Critic Loss: 0.0\n            =========================================\n        \n\n            =========================================\n            Timesteps: 3,818 / 2,000,000 (0.1909%)\n            Episodes: 6\n            Currently: Rollout\n            Latest Reward: -448\n            Latest Avg Rewards: -294\n            Recent Change: 33.12\n            Best Reward: -75.53\n            Latest Discrete Actor Loss: 0.0\n            Latest Continuous Actor Loss: 0.0\n            Avg Discrete Actor Loss: 0.0\n            Avg Continuous Actor Loss: 0.0\n            Latest Critic Loss: 0.0\n            Avg Critic Loss: 0.0\n            =========================================\n        \n","output_type":"stream"},{"name":"stdout","text":"Object 0 dropped to the floor\nb3Printf: b3Warning[examples/SharedMemory/PhysicsClientSharedMemory.cpp,1343]:\n\nb3Printf: Removing body failed\n","output_type":"stream"},{"name":"stderr","text":"\n            =========================================\n            Timesteps: 3,923 / 2,000,000 (0.1962%)\n            Episodes: 7\n            Currently: Rollout\n            Latest Reward: -79\n            Latest Avg Rewards: -263\n            Recent Change: -2.31\n            Best Reward: -75.53\n            Latest Discrete Actor Loss: 0.0\n            Latest Continuous Actor Loss: 0.0\n            Avg Discrete Actor Loss: 0.0\n            Avg Continuous Actor Loss: 0.0\n            Latest Critic Loss: 0.0\n            Avg Critic Loss: 0.0\n            =========================================\n        \n","output_type":"stream"},{"name":"stdout","text":"Object 0 dropped to the floor\nb3Printf: b3Warning[examples/SharedMemory/PhysicsClientSharedMemory.cpp,1343]:\n\nb3Printf: Removing body failed\n","output_type":"stream"},{"name":"stderr","text":"\n            =========================================\n            Timesteps: 4,026 / 2,000,000 (0.2013%)\n            Episodes: 8\n            Currently: Rollout\n            Latest Reward: -95\n            Latest Avg Rewards: -242\n            Recent Change: -15.52\n            Best Reward: -75.53\n            Latest Discrete Actor Loss: 0.0\n            Latest Continuous Actor Loss: 0.0\n            Avg Discrete Actor Loss: 0.0\n            Avg Continuous Actor Loss: 0.0\n            Latest Critic Loss: 0.0\n            Avg Critic Loss: 0.0\n            =========================================\n        \n\n            =========================================\n            Timesteps: 4,776 / 2,000,000 (0.2388%)\n            Episodes: 9\n            Currently: Rollout\n            Latest Reward: -300\n            Latest Avg Rewards: -249\n            Recent Change: -7.04\n            Best Reward: -75.53\n            Latest Discrete Actor Loss: 0.0\n            Latest Continuous Actor Loss: 0.0\n            Avg Discrete Actor Loss: 0.0\n            Avg Continuous Actor Loss: 0.0\n            Latest Critic Loss: 0.0\n            Avg Critic Loss: 0.0\n            =========================================\n        \n\n            =========================================\n            Timesteps: 5,526 / 2,000,000 (0.2763%)\n            Episodes: 10\n            Currently: Rollout\n            Latest Reward: -422\n            Latest Avg Rewards: -266\n            Recent Change: 4.32\n            Best Reward: -75.53\n            Latest Discrete Actor Loss: 0.0\n            Latest Continuous Actor Loss: 0.0\n            Avg Discrete Actor Loss: 0.0\n            Avg Continuous Actor Loss: 0.0\n            Latest Critic Loss: 0.0\n            Avg Critic Loss: 0.0\n            =========================================\n        \n\n            =========================================\n            Timesteps: 5,526 / 2,000,000 (0.2763%)\n            Episodes: 10\n            Currently: Training cycle 1/5\n            Latest Reward: -422\n            Latest Avg Rewards: -266\n            Recent Change: 4.32\n            Best Reward: -75.53\n            Latest Discrete Actor Loss: 0.0\n            Latest Continuous Actor Loss: 0.0\n            Avg Discrete Actor Loss: 0.0\n            Avg Continuous Actor Loss: 0.0\n            Latest Critic Loss: 0.0\n            Avg Critic Loss: 0.0\n            =========================================\n        \n/kaggle/working/RBE3043-23/src/trainer.py:380: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  advantage = (torch.tensor(rewards, dtype=torch.float32,\n/kaggle/working/RBE3043-23/src/trainer.py:446: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  reward_tensor = torch.tensor(rewards, dtype=torch.float32,\n\n            =========================================\n            Timesteps: 5,526 / 2,000,000 (0.2763%)\n            Episodes: 10\n            Currently: Training cycle 2/5\n            Latest Reward: -422\n            Latest Avg Rewards: -266\n            Recent Change: 4.32\n            Best Reward: -75.53\n            Latest Discrete Actor Loss: 0.0\n            Latest Continuous Actor Loss: 0.0052\n            Avg Discrete Actor Loss: 0.0\n            Avg Continuous Actor Loss: 0.0052\n            Latest Critic Loss: 1836.9104\n            Avg Critic Loss: 1836.9104\n            =========================================\n        \n\n            =========================================\n            Timesteps: 5,526 / 2,000,000 (0.2763%)\n            Episodes: 10\n            Currently: Training cycle 3/5\n            Latest Reward: -422\n            Latest Avg Rewards: -266\n            Recent Change: 4.32\n            Best Reward: -75.53\n            Latest Discrete Actor Loss: -0.0002\n            Latest Continuous Actor Loss: 0.0043\n            Avg Discrete Actor Loss: -0.0001\n            Avg Continuous Actor Loss: 0.0047\n            Latest Critic Loss: 1835.064\n            Avg Critic Loss: 1835.9872\n            =========================================\n        \n\n            =========================================\n            Timesteps: 5,526 / 2,000,000 (0.2763%)\n            Episodes: 10\n            Currently: Training cycle 4/5\n            Latest Reward: -422\n            Latest Avg Rewards: -266\n            Recent Change: 4.32\n            Best Reward: -75.53\n            Latest Discrete Actor Loss: -0.0004\n            Latest Continuous Actor Loss: 0.0035\n            Avg Discrete Actor Loss: -0.0002\n            Avg Continuous Actor Loss: 0.0043\n            Latest Critic Loss: 1833.0824\n            Avg Critic Loss: 1835.0189\n            =========================================\n        \n\n            =========================================\n            Timesteps: 5,526 / 2,000,000 (0.2763%)\n            Episodes: 10\n            Currently: Training cycle 5/5\n            Latest Reward: -422\n            Latest Avg Rewards: -266\n            Recent Change: 4.32\n            Best Reward: -75.53\n            Latest Discrete Actor Loss: -0.0005\n            Latest Continuous Actor Loss: 0.0029\n            Avg Discrete Actor Loss: -0.0003\n            Avg Continuous Actor Loss: 0.004\n            Latest Critic Loss: 1831.0699\n            Avg Critic Loss: 1834.0317\n            =========================================\n        \n","output_type":"stream"},{"name":"stdout","text":"Object 0 dropped to the floor\nb3Printf: b3Warning[examples/SharedMemory/PhysicsClientSharedMemory.cpp,1343]:\n\nb3Printf: Removing body failed\n","output_type":"stream"},{"name":"stderr","text":"\n            =========================================\n            Timesteps: 5,599 / 2,000,000 (0.2799%)\n            Episodes: 11\n            Currently: Rollout\n            Latest Reward: -81\n            Latest Avg Rewards: -249\n            Recent Change: -5.18\n            Best Reward: -75.53\n            Latest Discrete Actor Loss: -0.0007\n            Latest Continuous Actor Loss: 0.0025\n            Avg Discrete Actor Loss: -0.0004\n            Avg Continuous Actor Loss: 0.0037\n            Latest Critic Loss: 1829.0818\n            Avg Critic Loss: 1833.0417\n            =========================================\n        \n\n            =========================================\n            Timesteps: 6,349 / 2,000,000 (0.3174%)\n            Episodes: 12\n            Currently: Rollout\n            Latest Reward: -333\n            Latest Avg Rewards: -256\n            Recent Change: -0.75\n            Best Reward: -75.53\n            Latest Discrete Actor Loss: -0.0007\n            Latest Continuous Actor Loss: 0.0025\n            Avg Discrete Actor Loss: -0.0004\n            Avg Continuous Actor Loss: 0.0037\n            Latest Critic Loss: 1829.0818\n            Avg Critic Loss: 1833.0417\n            =========================================\n        \n","output_type":"stream"},{"name":"stdout","text":"Object 0 dropped to the floor\nb3Printf: b3Warning[examples/SharedMemory/PhysicsClientSharedMemory.cpp,1343]:\n\nb3Printf: Removing body failed\n","output_type":"stream"},{"name":"stderr","text":"\n            =========================================\n            Timesteps: 6,413 / 2,000,000 (0.3206%)\n            Episodes: 13\n            Currently: Rollout\n            Latest Reward: -72\n            Latest Avg Rewards: -242\n            Recent Change: -6.67\n            Best Reward: -71.66\n            Latest Discrete Actor Loss: -0.0007\n            Latest Continuous Actor Loss: 0.0025\n            Avg Discrete Actor Loss: -0.0004\n            Avg Continuous Actor Loss: 0.0037\n            Latest Critic Loss: 1829.0818\n            Avg Critic Loss: 1833.0417\n            =========================================\n        \n\n            =========================================\n            Timesteps: 7,163 / 2,000,000 (0.3582%)\n            Episodes: 14\n            Currently: Rollout\n            Latest Reward: -362\n            Latest Avg Rewards: -251\n            Recent Change: -1.9\n            Best Reward: -71.66\n            Latest Discrete Actor Loss: -0.0007\n            Latest Continuous Actor Loss: 0.0025\n            Avg Discrete Actor Loss: -0.0004\n            Avg Continuous Actor Loss: 0.0037\n            Latest Critic Loss: 1829.0818\n            Avg Critic Loss: 1833.0417\n            =========================================\n        \n","output_type":"stream"},{"name":"stdout","text":"Object 0 dropped to the floor\nb3Printf: b3Warning[examples/SharedMemory/PhysicsClientSharedMemory.cpp,1343]:\n\nb3Printf: Removing body failed\n","output_type":"stream"},{"name":"stderr","text":"\n            =========================================\n            Timesteps: 7,188 / 2,000,000 (0.3594%)\n            Episodes: 15\n            Currently: Rollout\n            Latest Reward: -60\n            Latest Avg Rewards: -238\n            Recent Change: -6.29\n            Best Reward: -60.42\n            Latest Discrete Actor Loss: -0.0007\n            Latest Continuous Actor Loss: 0.0025\n            Avg Discrete Actor Loss: -0.0004\n            Avg Continuous Actor Loss: 0.0037\n            Latest Critic Loss: 1829.0818\n            Avg Critic Loss: 1833.0417\n            =========================================\n        \n","output_type":"stream"},{"name":"stdout","text":"Object 0 incorrectly sorted into sorting_one\nb3Printf: b3Warning[examples/SharedMemory/PhysicsClientSharedMemory.cpp,1343]:\n\nb3Printf: Removing body failed\n","output_type":"stream"},{"name":"stderr","text":"\n            =========================================\n            Timesteps: 7,580 / 2,000,000 (0.379%)\n            Episodes: 16\n            Currently: Rollout\n            Latest Reward: -123\n            Latest Avg Rewards: -231\n            Recent Change: -7.71\n            Best Reward: -60.42\n            Latest Discrete Actor Loss: -0.0007\n            Latest Continuous Actor Loss: 0.0025\n            Avg Discrete Actor Loss: -0.0004\n            Avg Continuous Actor Loss: 0.0037\n            Latest Critic Loss: 1829.0818\n            Avg Critic Loss: 1833.0417\n            =========================================\n        \n\n            =========================================\n            Timesteps: 8,330 / 2,000,000 (0.4165%)\n            Episodes: 17\n            Currently: Rollout\n            Latest Reward: -230\n            Latest Avg Rewards: -231\n            Recent Change: -6.44\n            Best Reward: -60.42\n            Latest Discrete Actor Loss: -0.0007\n            Latest Continuous Actor Loss: 0.0025\n            Avg Discrete Actor Loss: -0.0004\n            Avg Continuous Actor Loss: 0.0037\n            Latest Critic Loss: 1829.0818\n            Avg Critic Loss: 1833.0417\n            =========================================\n        \n","output_type":"stream"},{"name":"stdout","text":"Object 0 dropped to the floor\nb3Printf: b3Warning[examples/SharedMemory/PhysicsClientSharedMemory.cpp,1343]:\n\nb3Printf: Removing body failed\n","output_type":"stream"},{"name":"stderr","text":"\n            =========================================\n            Timesteps: 8,465 / 2,000,000 (0.4233%)\n            Episodes: 18\n            Currently: Rollout\n            Latest Reward: -96\n            Latest Avg Rewards: -223\n            Recent Change: -7.78\n            Best Reward: -60.42\n            Latest Discrete Actor Loss: -0.0007\n            Latest Continuous Actor Loss: 0.0025\n            Avg Discrete Actor Loss: -0.0004\n            Avg Continuous Actor Loss: 0.0037\n            Latest Critic Loss: 1829.0818\n            Avg Critic Loss: 1833.0417\n            =========================================\n        \n","output_type":"stream"},{"name":"stdout","text":"Object 0 dropped to the floor\nb3Printf: b3Warning[examples/SharedMemory/PhysicsClientSharedMemory.cpp,1343]:\n\nb3Printf: Removing body failed\n","output_type":"stream"},{"name":"stderr","text":"\n            =========================================\n            Timesteps: 8,827 / 2,000,000 (0.4413%)\n            Episodes: 19\n            Currently: Rollout\n            Latest Reward: -164\n            Latest Avg Rewards: -220\n            Recent Change: -7.55\n            Best Reward: -60.42\n            Latest Discrete Actor Loss: -0.0007\n            Latest Continuous Actor Loss: 0.0025\n            Avg Discrete Actor Loss: -0.0004\n            Avg Continuous Actor Loss: 0.0037\n            Latest Critic Loss: 1829.0818\n            Avg Critic Loss: 1833.0417\n            =========================================\n        \n\n            =========================================\n            Timesteps: 9,577 / 2,000,000 (0.4788%)\n            Episodes: 20\n            Currently: Rollout\n            Latest Reward: -235\n            Latest Avg Rewards: -221\n            Recent Change: -6.26\n            Best Reward: -60.42\n            Latest Discrete Actor Loss: -0.0007\n            Latest Continuous Actor Loss: 0.0025\n            Avg Discrete Actor Loss: -0.0004\n            Avg Continuous Actor Loss: 0.0037\n            Latest Critic Loss: 1829.0818\n            Avg Critic Loss: 1833.0417\n            =========================================\n        \n\n            =========================================\n            Timesteps: 10,327 / 2,000,000 (0.5163%)\n            Episodes: 21\n            Currently: Rollout\n            Latest Reward: -235\n            Latest Avg Rewards: -221\n            Recent Change: -5.22\n            Best Reward: -60.42\n            Latest Discrete Actor Loss: -0.0007\n            Latest Continuous Actor Loss: 0.0025\n            Avg Discrete Actor Loss: -0.0004\n            Avg Continuous Actor Loss: 0.0037\n            Latest Critic Loss: 1829.0818\n            Avg Critic Loss: 1833.0417\n            =========================================\n        \n\n            =========================================\n            Timesteps: 11,077 / 2,000,000 (0.5538%)\n            Episodes: 22\n            Currently: Rollout\n            Latest Reward: -315\n            Latest Avg Rewards: -226\n            Recent Change: -3.43\n            Best Reward: -60.42\n            Latest Discrete Actor Loss: -0.0007\n            Latest Continuous Actor Loss: 0.0025\n            Avg Discrete Actor Loss: -0.0004\n            Avg Continuous Actor Loss: 0.0037\n            Latest Critic Loss: 1829.0818\n            Avg Critic Loss: 1833.0417\n            =========================================\n        \n\n            =========================================\n            Timesteps: 11,077 / 2,000,000 (0.5538%)\n            Episodes: 22\n            Currently: Training cycle 1/5\n            Latest Reward: -315\n            Latest Avg Rewards: -226\n            Recent Change: -3.43\n            Best Reward: -60.42\n            Latest Discrete Actor Loss: -0.0007\n            Latest Continuous Actor Loss: 0.0025\n            Avg Discrete Actor Loss: -0.0004\n            Avg Continuous Actor Loss: 0.0037\n            Latest Critic Loss: 1829.0818\n            Avg Critic Loss: 1833.0417\n            =========================================\n        \n\n            =========================================\n            Timesteps: 11,077 / 2,000,000 (0.5538%)\n            Episodes: 22\n            Currently: Training cycle 2/5\n            Latest Reward: -315\n            Latest Avg Rewards: -226\n            Recent Change: -3.43\n            Best Reward: -60.42\n            Latest Discrete Actor Loss: 0.0\n            Latest Continuous Actor Loss: 0.0002\n            Avg Discrete Actor Loss: -0.0003\n            Avg Continuous Actor Loss: 0.0031\n            Latest Critic Loss: 1289.1877\n            Avg Critic Loss: 1742.3994\n            =========================================\n        \n\n            =========================================\n            Timesteps: 11,077 / 2,000,000 (0.5538%)\n            Episodes: 22\n            Currently: Training cycle 3/5\n            Latest Reward: -315\n            Latest Avg Rewards: -226\n            Recent Change: -3.43\n            Best Reward: -60.42\n            Latest Discrete Actor Loss: -0.0004\n            Latest Continuous Actor Loss: 0.0001\n            Avg Discrete Actor Loss: -0.0003\n            Avg Continuous Actor Loss: 0.0027\n            Latest Critic Loss: 1287.5784\n            Avg Critic Loss: 1677.4249\n            =========================================\n        \n\n            =========================================\n            Timesteps: 11,077 / 2,000,000 (0.5538%)\n            Episodes: 22\n            Currently: Training cycle 4/5\n            Latest Reward: -315\n            Latest Avg Rewards: -226\n            Recent Change: -3.43\n            Best Reward: -60.42\n            Latest Discrete Actor Loss: -0.0007\n            Latest Continuous Actor Loss: 0.0001\n            Avg Discrete Actor Loss: -0.0004\n            Avg Continuous Actor Loss: 0.0023\n            Latest Critic Loss: 1285.8589\n            Avg Critic Loss: 1628.4792\n            =========================================\n        \n\n            =========================================\n            Timesteps: 11,077 / 2,000,000 (0.5538%)\n            Episodes: 22\n            Currently: Training cycle 5/5\n            Latest Reward: -315\n            Latest Avg Rewards: -226\n            Recent Change: -3.43\n            Best Reward: -60.42\n            Latest Discrete Actor Loss: -0.0011\n            Latest Continuous Actor Loss: 0.0\n            Avg Discrete Actor Loss: -0.0004\n            Avg Continuous Actor Loss: 0.0021\n            Latest Critic Loss: 1284.0846\n            Avg Critic Loss: 1590.2131\n            =========================================\n        \n","output_type":"stream"},{"name":"stdout","text":"Object 0 dropped to the floor\nb3Printf: b3Warning[examples/SharedMemory/PhysicsClientSharedMemory.cpp,1343]:\n\nb3Printf: Removing body failed\n","output_type":"stream"},{"name":"stderr","text":"\n            =========================================\n            Timesteps: 11,182 / 2,000,000 (0.5591%)\n            Episodes: 23\n            Currently: Rollout\n            Latest Reward: -86\n            Latest Avg Rewards: -220\n            Recent Change: -4.53\n            Best Reward: -60.42\n            Latest Discrete Actor Loss: -0.0016\n            Latest Continuous Actor Loss: 0.0\n            Avg Discrete Actor Loss: -0.0006\n            Avg Continuous Actor Loss: 0.0019\n            Latest Critic Loss: 1282.2802\n            Avg Critic Loss: 1559.4198\n            =========================================\n        \n","output_type":"stream"},{"name":"stdout","text":"Object 0 dropped to the floor\nb3Printf: b3Warning[examples/SharedMemory/PhysicsClientSharedMemory.cpp,1343]:\n\nb3Printf: Removing body failed\n","output_type":"stream"},{"name":"stderr","text":"\n            =========================================\n            Timesteps: 11,625 / 2,000,000 (0.5813%)\n            Episodes: 24\n            Currently: Rollout\n            Latest Reward: -207\n            Latest Avg Rewards: -219\n            Recent Change: -4.11\n            Best Reward: -60.42\n            Latest Discrete Actor Loss: -0.0016\n            Latest Continuous Actor Loss: 0.0\n            Avg Discrete Actor Loss: -0.0006\n            Avg Continuous Actor Loss: 0.0019\n            Latest Critic Loss: 1282.2802\n            Avg Critic Loss: 1559.4198\n            =========================================\n        \n\n            =========================================\n            Timesteps: 12,375 / 2,000,000 (0.6188%)\n            Episodes: 25\n            Currently: Rollout\n            Latest Reward: -279\n            Latest Avg Rewards: -222\n            Recent Change: -3.08\n            Best Reward: -60.42\n            Latest Discrete Actor Loss: -0.0016\n            Latest Continuous Actor Loss: 0.0\n            Avg Discrete Actor Loss: -0.0006\n            Avg Continuous Actor Loss: 0.0019\n            Latest Critic Loss: 1282.2802\n            Avg Critic Loss: 1559.4198\n            =========================================\n        \n\n            =========================================\n            Timesteps: 13,125 / 2,000,000 (0.6562%)\n            Episodes: 26\n            Currently: Rollout\n            Latest Reward: -271\n            Latest Avg Rewards: -223\n            Recent Change: -2.32\n            Best Reward: -60.42\n            Latest Discrete Actor Loss: -0.0016\n            Latest Continuous Actor Loss: 0.0\n            Avg Discrete Actor Loss: -0.0006\n            Avg Continuous Actor Loss: 0.0019\n            Latest Critic Loss: 1282.2802\n            Avg Critic Loss: 1559.4198\n            =========================================\n        \n\n            =========================================\n            Timesteps: 13,875 / 2,000,000 (0.6937%)\n            Episodes: 27\n            Currently: Rollout\n            Latest Reward: -312\n            Latest Avg Rewards: -227\n            Recent Change: -1.37\n            Best Reward: -60.42\n            Latest Discrete Actor Loss: -0.0016\n            Latest Continuous Actor Loss: 0.0\n            Avg Discrete Actor Loss: -0.0006\n            Avg Continuous Actor Loss: 0.0019\n            Latest Critic Loss: 1282.2802\n            Avg Critic Loss: 1559.4198\n            =========================================\n        \n","output_type":"stream"},{"name":"stdout","text":"Object 0 incorrectly sorted into sorting_one\nb3Printf: b3Warning[examples/SharedMemory/PhysicsClientSharedMemory.cpp,1343]:\n\nb3Printf: Removing body failed\n","output_type":"stream"},{"name":"stderr","text":"\n            =========================================\n            Timesteps: 14,145 / 2,000,000 (0.7072%)\n            Episodes: 28\n            Currently: Rollout\n            Latest Reward: -120\n            Latest Avg Rewards: -223\n            Recent Change: -2.01\n            Best Reward: -60.42\n            Latest Discrete Actor Loss: -0.0016\n            Latest Continuous Actor Loss: 0.0\n            Avg Discrete Actor Loss: -0.0006\n            Avg Continuous Actor Loss: 0.0019\n            Latest Critic Loss: 1282.2802\n            Avg Critic Loss: 1559.4198\n            =========================================\n        \n","output_type":"stream"},{"name":"stdout","text":"Object 0 dropped to the floor\nb3Printf: b3Warning[examples/SharedMemory/PhysicsClientSharedMemory.cpp,1343]:\n\nb3Printf: Removing body failed\n","output_type":"stream"},{"name":"stderr","text":"\n            =========================================\n            Timesteps: 14,340 / 2,000,000 (0.717%)\n            Episodes: 29\n            Currently: Rollout\n            Latest Reward: -104\n            Latest Avg Rewards: -219\n            Recent Change: -2.63\n            Best Reward: -60.42\n            Latest Discrete Actor Loss: -0.0016\n            Latest Continuous Actor Loss: 0.0\n            Avg Discrete Actor Loss: -0.0006\n            Avg Continuous Actor Loss: 0.0019\n            Latest Critic Loss: 1282.2802\n            Avg Critic Loss: 1559.4198\n            =========================================\n        \n\n            =========================================\n            Timesteps: 15,090 / 2,000,000 (0.7545%)\n            Episodes: 30\n            Currently: Rollout\n            Latest Reward: -228\n            Latest Avg Rewards: -219\n            Recent Change: -2.32\n            Best Reward: -60.42\n            Latest Discrete Actor Loss: -0.0016\n            Latest Continuous Actor Loss: 0.0\n            Avg Discrete Actor Loss: -0.0006\n            Avg Continuous Actor Loss: 0.0019\n            Latest Critic Loss: 1282.2802\n            Avg Critic Loss: 1559.4198\n            =========================================\n        \n\n            =========================================\n            Timesteps: 15,840 / 2,000,000 (0.792%)\n            Episodes: 31\n            Currently: Rollout\n            Latest Reward: -248\n            Latest Avg Rewards: -220\n            Recent Change: -1.93\n            Best Reward: -60.42\n            Latest Discrete Actor Loss: -0.0016\n            Latest Continuous Actor Loss: 0.0\n            Avg Discrete Actor Loss: -0.0006\n            Avg Continuous Actor Loss: 0.0019\n            Latest Critic Loss: 1282.2802\n            Avg Critic Loss: 1559.4198\n            =========================================\n        \n\n            =========================================\n            Timesteps: 16,590 / 2,000,000 (0.8295%)\n            Episodes: 32\n            Currently: Rollout\n            Latest Reward: -343\n            Latest Avg Rewards: -224\n            Recent Change: -1.05\n            Best Reward: -60.42\n            Latest Discrete Actor Loss: -0.0016\n            Latest Continuous Actor Loss: 0.0\n            Avg Discrete Actor Loss: -0.0006\n            Avg Continuous Actor Loss: 0.0019\n            Latest Critic Loss: 1282.2802\n            Avg Critic Loss: 1559.4198\n            =========================================\n        \n\n            =========================================\n            Timesteps: 16,590 / 2,000,000 (0.8295%)\n            Episodes: 32\n            Currently: Training cycle 1/5\n            Latest Reward: -343\n            Latest Avg Rewards: -224\n            Recent Change: -1.05\n            Best Reward: -60.42\n            Latest Discrete Actor Loss: -0.0016\n            Latest Continuous Actor Loss: 0.0\n            Avg Discrete Actor Loss: -0.0006\n            Avg Continuous Actor Loss: 0.0019\n            Latest Critic Loss: 1282.2802\n            Avg Critic Loss: 1559.4198\n            =========================================\n        \n\n            =========================================\n            Timesteps: 16,590 / 2,000,000 (0.8295%)\n            Episodes: 32\n            Currently: Training cycle 2/5\n            Latest Reward: -343\n            Latest Avg Rewards: -224\n            Recent Change: -1.05\n            Best Reward: -60.42\n            Latest Discrete Actor Loss: 0.0\n            Latest Continuous Actor Loss: 0.0\n            Avg Discrete Actor Loss: -0.0005\n            Avg Continuous Actor Loss: 0.0017\n            Latest Critic Loss: 1246.3691\n            Avg Critic Loss: 1530.9607\n            =========================================\n        \n\n            =========================================\n            Timesteps: 16,590 / 2,000,000 (0.8295%)\n            Episodes: 32\n            Currently: Training cycle 3/5\n            Latest Reward: -343\n            Latest Avg Rewards: -224\n            Recent Change: -1.05\n            Best Reward: -60.42\n            Latest Discrete Actor Loss: 0.0\n            Latest Continuous Actor Loss: 0.0\n            Avg Discrete Actor Loss: -0.0005\n            Avg Continuous Actor Loss: 0.0016\n            Latest Critic Loss: 1244.3475\n            Avg Critic Loss: 1507.0762\n            =========================================\n        \n\n            =========================================\n            Timesteps: 16,590 / 2,000,000 (0.8295%)\n            Episodes: 32\n            Currently: Training cycle 4/5\n            Latest Reward: -343\n            Latest Avg Rewards: -224\n            Recent Change: -1.05\n            Best Reward: -60.42\n            Latest Discrete Actor Loss: -0.0001\n            Latest Continuous Actor Loss: 0.0\n            Avg Discrete Actor Loss: -0.0004\n            Avg Continuous Actor Loss: 0.0015\n            Latest Critic Loss: 1242.2148\n            Avg Critic Loss: 1486.7023\n            =========================================\n        \n\n            =========================================\n            Timesteps: 16,590 / 2,000,000 (0.8295%)\n            Episodes: 32\n            Currently: Training cycle 5/5\n            Latest Reward: -343\n            Latest Avg Rewards: -224\n            Recent Change: -1.05\n            Best Reward: -60.42\n            Latest Discrete Actor Loss: -0.0002\n            Latest Continuous Actor Loss: 0.0\n            Avg Discrete Actor Loss: -0.0004\n            Avg Continuous Actor Loss: 0.0013\n            Latest Critic Loss: 1239.9762\n            Avg Critic Loss: 1469.079\n            =========================================\n        \n\n            =========================================\n            Timesteps: 17,340 / 2,000,000 (0.867%)\n            Episodes: 33\n            Currently: Rollout\n            Latest Reward: -263\n            Latest Avg Rewards: -225\n            Recent Change: -0.75\n            Best Reward: -60.42\n            Latest Discrete Actor Loss: -0.0004\n            Latest Continuous Actor Loss: 0.0\n            Avg Discrete Actor Loss: -0.0004\n            Avg Continuous Actor Loss: 0.0013\n            Latest Critic Loss: 1237.6031\n            Avg Critic Loss: 1453.6473\n            =========================================\n        \n\n            =========================================\n            Timesteps: 18,090 / 2,000,000 (0.9045%)\n            Episodes: 34\n            Currently: Rollout\n            Latest Reward: -402\n            Latest Avg Rewards: -230\n            Recent Change: 0.2\n            Best Reward: -60.42\n            Latest Discrete Actor Loss: -0.0004\n            Latest Continuous Actor Loss: 0.0\n            Avg Discrete Actor Loss: -0.0004\n            Avg Continuous Actor Loss: 0.0013\n            Latest Critic Loss: 1237.6031\n            Avg Critic Loss: 1453.6473\n            =========================================\n        \n","output_type":"stream"},{"name":"stdout","text":"Object 0 incorrectly sorted into sorting_one\nb3Printf: b3Warning[examples/SharedMemory/PhysicsClientSharedMemory.cpp,1343]:\n\nb3Printf: Removing body failed\n","output_type":"stream"},{"name":"stderr","text":"\n            =========================================\n            Timesteps: 18,153 / 2,000,000 (0.9076%)\n            Episodes: 35\n            Currently: Rollout\n            Latest Reward: -43\n            Latest Avg Rewards: -225\n            Recent Change: -0.7\n            Best Reward: -43.14\n            Latest Discrete Actor Loss: -0.0004\n            Latest Continuous Actor Loss: 0.0\n            Avg Discrete Actor Loss: -0.0004\n            Avg Continuous Actor Loss: 0.0013\n            Latest Critic Loss: 1237.6031\n            Avg Critic Loss: 1453.6473\n            =========================================\n        \n\n            =========================================\n            Timesteps: 18,903 / 2,000,000 (0.9451%)\n            Episodes: 36\n            Currently: Rollout\n            Latest Reward: -245\n            Latest Avg Rewards: -225\n            Recent Change: -0.55\n            Best Reward: -43.14\n            Latest Discrete Actor Loss: -0.0004\n            Latest Continuous Actor Loss: 0.0\n            Avg Discrete Actor Loss: -0.0004\n            Avg Continuous Actor Loss: 0.0013\n            Latest Critic Loss: 1237.6031\n            Avg Critic Loss: 1453.6473\n            =========================================\n        \n\n            =========================================\n            Timesteps: 19,653 / 2,000,000 (0.9827%)\n            Episodes: 37\n            Currently: Rollout\n            Latest Reward: -307\n            Latest Avg Rewards: -228\n            Recent Change: -0.16\n            Best Reward: -43.14\n            Latest Discrete Actor Loss: -0.0004\n            Latest Continuous Actor Loss: 0.0\n            Avg Discrete Actor Loss: -0.0004\n            Avg Continuous Actor Loss: 0.0013\n            Latest Critic Loss: 1237.6031\n            Avg Critic Loss: 1453.6473\n            =========================================\n        \n\n            =========================================\n            Timesteps: 20,403 / 2,000,000 (1.0202%)\n            Episodes: 38\n            Currently: Rollout\n            Latest Reward: -369\n            Latest Avg Rewards: -231\n            Recent Change: 0.42\n            Best Reward: -43.14\n            Latest Discrete Actor Loss: -0.0004\n            Latest Continuous Actor Loss: 0.0\n            Avg Discrete Actor Loss: -0.0004\n            Avg Continuous Actor Loss: 0.0013\n            Latest Critic Loss: 1237.6031\n            Avg Critic Loss: 1453.6473\n            =========================================\n        \n\n            =========================================\n            Timesteps: 21,153 / 2,000,000 (1.0576%)\n            Episodes: 39\n            Currently: Rollout\n            Latest Reward: -241\n            Latest Avg Rewards: -232\n            Recent Change: 0.43\n            Best Reward: -43.14\n            Latest Discrete Actor Loss: -0.0004\n            Latest Continuous Actor Loss: 0.0\n            Avg Discrete Actor Loss: -0.0004\n            Avg Continuous Actor Loss: 0.0013\n            Latest Critic Loss: 1237.6031\n            Avg Critic Loss: 1453.6473\n            =========================================\n        \n\n            =========================================\n            Timesteps: 21,903 / 2,000,000 (1.0951%)\n            Episodes: 40\n            Currently: Rollout\n            Latest Reward: -331\n            Latest Avg Rewards: -234\n            Recent Change: 0.76\n            Best Reward: -43.14\n            Latest Discrete Actor Loss: -0.0004\n            Latest Continuous Actor Loss: 0.0\n            Avg Discrete Actor Loss: -0.0004\n            Avg Continuous Actor Loss: 0.0013\n            Latest Critic Loss: 1237.6031\n            Avg Critic Loss: 1453.6473\n            =========================================\n        \n\n            =========================================\n            Timesteps: 21,903 / 2,000,000 (1.0951%)\n            Episodes: 40\n            Currently: Training cycle 1/5\n            Latest Reward: -331\n            Latest Avg Rewards: -234\n            Recent Change: 0.76\n            Best Reward: -43.14\n            Latest Discrete Actor Loss: -0.0004\n            Latest Continuous Actor Loss: 0.0\n            Avg Discrete Actor Loss: -0.0004\n            Avg Continuous Actor Loss: 0.0013\n            Latest Critic Loss: 1237.6031\n            Avg Critic Loss: 1453.6473\n            =========================================\n        \n\n            =========================================\n            Timesteps: 21,903 / 2,000,000 (1.0951%)\n            Episodes: 40\n            Currently: Training cycle 2/5\n            Latest Reward: -331\n            Latest Avg Rewards: -234\n            Recent Change: 0.76\n            Best Reward: -43.14\n            Latest Discrete Actor Loss: 0.0\n            Latest Continuous Actor Loss: 0.0001\n            Avg Discrete Actor Loss: -0.0004\n            Avg Continuous Actor Loss: 0.0012\n            Latest Critic Loss: 1477.248\n            Avg Critic Loss: 1455.1223\n            =========================================\n        \n\n            =========================================\n            Timesteps: 21,903 / 2,000,000 (1.0951%)\n            Episodes: 40\n            Currently: Training cycle 3/5\n            Latest Reward: -331\n            Latest Avg Rewards: -234\n            Recent Change: 0.76\n            Best Reward: -43.14\n            Latest Discrete Actor Loss: -0.0001\n            Latest Continuous Actor Loss: 0.0001\n            Avg Discrete Actor Loss: -0.0004\n            Avg Continuous Actor Loss: 0.0011\n            Latest Critic Loss: 1474.4113\n            Avg Critic Loss: 1456.257\n            =========================================\n        \n","output_type":"stream"}],"execution_count":null}]}